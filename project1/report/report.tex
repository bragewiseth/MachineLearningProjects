\documentclass[twoside,11pt]{report}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
% \usepackage[utf8]{inputenc}%
% \usepackage{tikz}
% \usepackage{cfr-lm}%
\usepackage[T1]{fontenc}%
\usepackage{physics}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage[margin=3cm]{geometry}
% \usepackage{changepage}
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{lettrine}
% \usepackage{fontawesome}
\usemintedstyle{perldoc}
\usepackage{hyperref}
\hypersetup{colorlinks=false, pdfborder={0 0 0},  }
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{adjustbox}



\newtcbox{\codebox}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1pt, boxsep=0pt,left=2pt,right=2pt,top=1pt,bottom=.5pt}
\newtcbox{\deloppg}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-2pt]{0pt}{0pt}},boxrule=0pt, boxsep=0pt,left=.49\linewidth,right=.49\linewidth,top=4pt,bottom=3pt}


\newcommand\blfootnote[1]{ \begingroup \renewcommand\thefootnote{}\footnote{#1} \addtocounter{footnote}{-1} \endgroup }
% \definecolor{antwhite}{HTML}{323333}
\newcommand{\code}[3][]{\codebox{\mintinline[#1]{#2}{#3}}}



% \setmainfont{FreeSans}
% \setmainfont{SF Pro Display}
% \setmainfont{IBM Plex Sans}
% \setmainfont{TeX Gyre Heros}
% \setmainfont{Inter}
% \setmainfont{Iosevka Quasi}

% \setmonofont{Iosevka Custom Extended}
% \setmonofont{JetBrainsMono Nerd Font}
% \setmonofont[Scale=MatchLowercase]{SF-Mono-Medium}





% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{https://github.com/bragewiseth/MachineLearningProjects}

% Short headings should be running head and authors last names

\ShortHeadings{\url{https://github.com/bragewiseth/MachineLearningProjects}}{\url{https://github.com/bragewiseth/MachineLearningProjects}}
\firstpageno{1}



\title{{\huge Project 1}}
\author{\name Eirik J. \email eirk@ifi.uio.no \\
       \name Felix H.  \email felixch@ifi.uio.no \\
       \name Brage W. \email bragewi@ifi.uio.no}
\date{\today}											% Date
\makeatletter






% \date{\today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{uio.jpg}\\[1.0 cm]	% University Logo
    \textsc{\LARGE University of Oslo}\\[2.0 cm]	    % University Name
	\textsc{\Large FYS-STK3155}\\[0.5 cm]				% Course Code
	\rule{\linewidth}{0.2 mm} \\[0.4 cm]
	{ \huge \bfseries \@title}\\
	\rule{\linewidth}{0.2 mm} \\[1.5 cm]

	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \normalsize
			Eirik\\
            Felix\\
            Brage Wiseth\\
			\end{flushleft}
			\end{minipage}~
			\begin{minipage}{0.4\textwidth}
			\begin{flushright} \normalsize
        \textsc{
		  eirik@ifi.uio.no\\
          felix@ifi.uio.no\\
          bragewi@ifi.uio.no\\
        }
		\end{flushright}
        
	\end{minipage}\\[2 cm]
	\@date\\
    \vspace*{25mm}
    \urlstyle{rm}
    \textsc{\url{https://github.com/bragewiseth/MachineLearningProjects}}
	
	
    
    
    
    
	
\end{titlepage}
\nocite{*}
% \maketitle
\newpage
\tableofcontents
\newpage

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\lettrine{I}{}n this paper, we delve into the realm of machine learning model optimization and evaluation. 
Our study encompasses various regression techniques, including Ordinary Least Squares (OLS), Ridge, and 
Lasso regression, to analyze their effectiveness in handling simple and more complex datasets. Additionally, 
we employ bootstrap resampling and cross-validation methodologies to rigorously assess model performance 
and enhance generalization.
A significant portion of our investigation is dedicated to understanding the delicate balance between 
bias and variance. We explore how regularization methods like Ridge and Lasso impact bias-variance trade-offs, 
offering insights into the stability and predictive power of these models. Furthermore, we provide empirical 
evidence of the benefits of cross-validation and bootstrap techniques in mitigating overfitting and improving 
model robustness. We found that \{ ..results.. \}. Additionally we verify and compare our findings with well 
established theory and libraris such as SKLearn.
\end{abstract}
\begin{keywords}
    Linear Regression, Scaling, Bias \& Variance 
\end{keywords}

\section{Introduction}

Machine learning has emerged as a powerful tool in data analysis, providing the ability to uncover complex 
patterns and relationships in diverse datasets. But, at its core, machine learning is all about finding 
functions that capture the underlying structure of the data. The use of machine learning algorithms 
to approximate functions is the essence of this paper.\\

Our motivation for this research lies in the exploration of machine learning techniques to approximate 
the terrain on our planet, which can perhaps be described by such a function. Earth's terrain exhibits 
peaks and troughs, hills and valleys, much like some polynomial functions. Fortunately, we can employ 
standard linear regression techniques to approximate polynomials, but the terrain presents its own set 
of challenges. Firstly, the terrain's true underlying function may not be a polynomial at all, and its complexity may 
vary significantly from one location to another. Secondly, our landscape is teeming with small, intricate 
details. Some regions are characterized by flat and smooth surfaces, while others are marked by rough and 
uneven terrain. Focusing too much on these minute details can lead to model overfitting, making it crucial 
to strike a careful balance between model complexity and generalization.
In this context, regularization and resampling techniques, including Ridge and Lasso regression with bootstrap
and cross validation, have proven indispensable. By introducing regularization and resampling, we aim to find 
the sweet spot between bias and Variance. And getting the best predictions we can with our assumptions.\\
To embark on this exploration, we will begin with a simpler case: "Franke's function." which mimics our real terrain
data. This function 
serves as a foundational starting point, allowing us to assess our model's performance in a controlled 
environment before venturing into the complexity of real-world terrain data. Through this gradual progression, 
we provide ourselves wih a framework that can be applied to more complex and varied real-world terrain datasets.\\

\textbf{Data}: We begin by introducing the dataset used for our analysis, highlighting data collection and preprocessing procedures. 
Understanding the characteristics of the terrain data is fundamental to our modeling endeavor.\\
\textbf{Methods and Scaling}: Next, we delve into the methodology, encompassing the implementation of polynomial regression 
models and the application of regularization techniques such as Ridge and Lasso. Additionally, we will discuss the 
importance of proper scaling for model stability and convergence.\\
\textbf{Bias-Variance Trade-off}: A significant portion of our study will revolve around the critical concept of bias and 
variance. We'll explore how regularization methods influence this trade-off and delve into the fine balance between 
model complexity and generalization.\\
\textbf{Results}: In this section, we will present the outcomes of our experiments, showcasing the performance of different 
models and regularization techniques. Through empirical evidence, we aim to provide insights into the effectiveness of our approach.\\
\textbf{Conclusion}: Finally, we will summarize the key findings and their implications for terrain modeling with machine learning. 
Our conclusion will underscore the importance of regularization in achieving accurate representations of complex terrains and 
provide a perspective on future research directions.




% \section{Data}
% \label{sec:Data}
%
% \begin{wrapfigure}{r}{0.3\textwidth}
% \begin{center}
%     \begin{adjustbox}{clip,trim=4cm 3.4cm 3cm 4.8cm, max width=0.3\textwidth}
%     \input{Frank_1.pgf}
%     \end{adjustbox}
%     \caption{\small Franke's Function with noise}
%     \label{frankfuncnoise}
%     \end{center}
% \end{wrapfigure}

% \begin{wrapfigure}{l}{0.3\textwidth}
%     \caption{\small Franke's Function}
%     \label{frankfunc}
%     \begin{center}
%     \begin{adjustbox}{clip,trim=4cm 3.4cm 3cm 4.8cm, max width=0.3\textwidth}
%     \input{Frank_2.pgf}
%     \end{adjustbox}
%     \end{center}
% \end{wrapfigure}
%
%
%
% \section{Linear Regression Models }
% \label{sec:models}
%
%
%
% $$
% MSE(\mathbf{y},\mathbf{\tilde{y}}) = \frac{1}{n}
% \sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2,
% $$
% $$
% R^2(\mathbf{y}, \tilde{\mathbf{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
% $$
%
% $$
% \bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
% $$
%
%
% \subsection{Ordinary Least Squares (OLS)}
% \label{sec:OLS}
%
% \subsection{Ridge Regression}
% \label{sec:Ridge}
%
% \subsection{Lasso Regression}
% \label{sec:Lasso}
%
%
% \begin{figure}[!h]
%     \begin{center}
%     \resizebox*{0.9\linewidth}{!}{\input{Figure_2.pgf}}
%     \caption{Franke's Function}
%     \label{ols}
%     {\scriptsize code for gen  output}\footnote{code for }
%     \end{center}
% \end{figure}
% \begin{figure}[!h]
%     \begin{center}
%     \resizebox*{0.7\linewidth}{!}{\input{beta-modelcomplexity.pgf}}
%     \caption{Franke's Function}
%     \label{}
%     \footnote{https}
%     \end{center}
% \end{figure}
%
%
% \section{Results and Discussion}
% \label{sec:ResultsDiscussion}
%
% As we increase the order of the polynomial fit we see a corresponding decrease in MSE and increase in R2.
%
%
% We see that OLS remains constant since it is not a function of λ. For the MSE from the training set we se that 
% the MSE wil increase for larger values of λ for rigde regression. This is expected since the OLS wil give the 
% best approximation for the data it has been trained on (compared to Rigde), while rigde punishes the weights 
% from becomming too large and therefore gives a not so tight fit. This can explain why rigde does better than 
% OLS for some values of λ. As stated before the OLS wil try to fit the model to the best of its ability to the 
% training data, this includes the noise which might not generalize to the test set, hence giving rigde a greater score. 
% The term for this is called ***overfitting***. And balancing between overfitting and underfitting is called the ***bias variance tradeoff***. 
%
% \subsection{Confidence Interval}
% \label{sec:ConfidenceInterval}
%
% The assumption we have made is that there exists a continuous function f (x)
% and a normal distributed error ε ∼ N (0, σ2) which describes our data
% y = f (x) + ε
% We then approximate this function f (x) with our model ˜y from the solution
% of the linear regression equations (ordinary least squares OLS), that is our
% function f is approximated by ˜y where we minimized (y − ˜y)2, with
% ˜y = Xβ.
% The matrix X is the so-called design or feature matrix.
% yi ∼ N (Xi,∗ β, σ2), that is y follows a normal distribution with mean
% value Xβ and variance σ2.
% We can use this when we define a so-called confidence interval
% for the parameters β. A given parameter βj is given by the diagonal matrix
% element of the above matrix.
% \ref{app:confidenceInterval}
%
%
%
% \section{The Topic of Scaling}
% \label{sec:Scaling}
%
%
% \begin{table}[!h]
% \caption{Unscaled sample design matrix fitting one-dimensional polynomial of degree 5}
% \tt
% \centering
% \resizebox{0.6\textwidth}{!}{%
% \begin{tabular}{llllll}
%     1. &     0.  &    0.  &    0.   &   0.   &   0.     \\
%     1. &     0.25 &   0.0625 & 0.01562& 0.00391& 0.00098\\
%     1.    &  0.5     &0.25 &   0.125 &  0.0625 & 0.03125\\
%     1.   &   0.75  &  0.5625 & 0.42188 &0.31641& 0.2373 \\
%     1.  &    1.   &   1.  &    1.  &    1.    &  1.
% \end{tabular}%
% }
% \end{table}
%
%
%
% \begin{table}[!h]
% \caption{Scaled sample design matrix fitting one-dimensional polynomial of degree 5}
% \tt
% \centering
% \resizebox{0.6\textwidth}{!}{%
% \begin{tabular}{llllll}
%      0. &     -1.41421& -1.0171&  -0.83189& -0.728 &  -0.66226\\
%      0.  &    -0.70711& -0.84758& -0.7903 & -0.71772& -0.65971\\
%      0.  &     0.    &  -0.33903& -0.49913& -0.56348& -0.58075\\
%      0.  &     0.70711&  0.50855&  0.29116 & 0.10488& -0.0433 \\
%      0.  &     1.41421&  1.69516 & 1.83016 & 1.90431&  1.94603
% \end{tabular}%
% }\\
% {\scriptsize the code for generating this output}
% \footnote{\url{https://github.com/bragewiseth/MachineLearningProjects/blob/main/project1/src/linearRegression.py}\cite{MachineLearningProjects_2023}}
% % \end{table}
%
%
%
% \begin{table}[h]
% \centering
% \resizebox{0.6\textwidth}{!}{%
% \begin{tabular}{ll}
% MSE for OLS on unscaled data:    &   \texttt{0.010349396022903145} \\
% MSE for OLS on scaled data:      &   \texttt{0.010349396024145656} \\
% MSE for Ridge on unscaled data:  &   \texttt{0.02106077418650843} \\
% MSE for Ridge on scaled data:    &   \texttt{0.01782525371566323}
% \end{tabular}%
% }\\
% {\scriptsize
% the code for generating this output }
% \end{table}
%
%
% First $x_{\text{unscaled}}$ and $x_{\text{scaled}}$ is not that different, the original 
% data was close to zero-centered and not that spread out, which means that initially by 
% just looking at the data scaling is not that necessary. When we add the polynomial
% terms we can now see that some of the entries of $X_{\text{unscaled}}$ get really small 
% as an example $0.1^5 = 0.00001$ this makes the columns of $X_{\text{unscaled}}$ live in 
% their own order of magnitude and scaling should be considered to bring them back to the 
% same order of magnitude. The act of not scaling results in $\mathbf{\beta}$ spanning 
% from $-50$ to $48$ while scaling gives a smaller span from $-10$ to $13$. Now this alone 
% may not justify why we should scale this data set, as scaled and unscaled OLS yields the 
% same MSE. However when doing ridge regression the cost function is directly dependent on 
% the magnitude of $\beta_i$. Now with each $\beta_i$ varying alot, some are getting more 
% penalized than others. As we can see the MSE for unscaled data is much higher than for 
% scaled data in the ridge case.
% This leads us to conclude that we should scale the data, making it easier to 
% tweak $\lambda$ and giving us nicer numbers to work with
%
%
%
%
%
%
%
% \section{Bias \& Variance}
% \label{sec:BiasVariance}
%
%
% \begin{align*}
%     \text{Var}(\mathbf{\tilde{y}}) &= \mathbb{E}\left[\left(\tilde{\mathbf{y}}-\mathbb{E}\left[\mathbf{\tilde{y}}\right]\right)^2\right] = \mathbb{E}[\mathbf{\tilde{y}}^2] - \mathbb{E}[\mathbf{\tilde{y}}]^2\\
%     \mathbb{E}[\mathbf{\tilde{y}}^2] &= \mathbb{E}[\mathbf{\tilde{y}}]^2 + \text{Var}(\mathbf{\tilde{y}})
% \end{align*}
%
%
% \begin{align*}
% \mathbb{E}[\mathbf{y}^2] & = \mathbb{E}[\mathbf{f} + \mathbf{\epsilon}]^2 = \mathbb{E}[\mathbf{f}^2 + 2\mathbf{f}\mathbf{\epsilon} + \mathbf{\epsilon}^2]\\
% & = \mathbb{E}[\mathbf{f}^2] + 2\mathbb{E}[\mathbf{f}\mathbf{\epsilon}] + \mathbb{E}[\mathbf{\epsilon}^2]\\
% & = \mathbb{E}[\mathbf{f}^2] + 2\mathbb{E}[\mathbf{f}]\mathbb{E}[\mathbf{\epsilon}] + \mathbb{E}[\mathbf{\epsilon}^2]\\
% & = \mathbb{E}[\mathbf{f}^2]  + \sigma^2
% \end{align*}
%
%
% \begin{align*}
% \mathbb{E}[\mathbf{y\tilde{y}}] & = \mathbb{E}[\mathbf{f\tilde{y}} + \mathbf{\epsilon\tilde{y}}]\\
% & = \mathbb{E}[\mathbf{f\tilde{y}}] + \mathbb{E}[\mathbf{\epsilon\tilde{y}}]\\
% & = \mathbb{E}[\mathbf{f\tilde{y}}] + \mathbb{E}[\mathbf{\epsilon}]\mathbb{E}[\mathbf{\tilde{y}}]\\
% & = \mathbf{f}\mathbb{E}[\mathbf{\tilde{y}}]
% \end{align*}
%
%
% \begin{align*}
% \mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right] & = \mathbb{E}[\mathbf{y}^2] - 2\mathbb{E}[\mathbf{y\tilde{y}}] + \mathbb{E}[\mathbf{\tilde{y}}^2]\\
% & = \mathbf{f}^2 + \sigma^2 - 2\mathbf{f}\mathbb{E}[\mathbf{\tilde{y}}] + \mathbb{E}[\mathbf{\tilde{y}}^2]\\
% & = \mathbf{f}^2 + \sigma^2 - 2\mathbf{f}\mathbb{E}[\mathbf{\tilde{y}}] + \mathbb{E}[\mathbf{\tilde{y}}]^2 + \text{Var}(\mathbf{\tilde{y}})\\
% & = (\mathbf{f} - \mathbb{E}[\mathbf{\tilde{y}}])^2 + \text{Var}(\mathbf{\tilde{y}}) + \sigma^2\\
% \end{align*}
%
%
% \begin{align*}
% \mathrm{Bias}[\tilde{y}]& =\mathbb{E}\left[\left(\mathbf{y}-\mathbb{E}\left[\mathbf{\tilde{y}}\right]\right)^2\right] = \mathbb{E}[\mathbf{y}^2] - 2\mathbb{E}[\mathbf{y}]\mathbb{E}[\mathbf{\tilde{y}}] + \mathbb{E}[\mathbf{\tilde{y}}^2]\\
% & = \mathbf{f}^2 + \sigma^2 - 2\mathbf{f}\mathbb{E}[\mathbf{\tilde{y}}] + \mathbb{E}[\mathbf{\tilde{y}}]^2\\
% & = (\mathbf{f} - \mathbb{E}[\mathbf{\tilde{y}}])^2 + \sigma^2\\
% \end{align*}
%
%
% To find $\mathbb{E}[\mathbf{\tilde{y}}]$ we can use bootstrap to get different samples for $\mathbf{\tilde{y}}$ and then take the mean of that distribution.
% The term \emph{variance} refers to how spread out our observations are. We can for our case think of the observations $\tilde{y}_i$
% as either individual predictions, or the mean of predictions from one test set.\\
% In a sense the bias resembles the mathematical defninition for variance, We can see that the bias term is a distance measure of $\mathbf{y}$ from the expected value of $\mathbf{\tilde{y}}$. If we take the individual observations approach this can be tough of as A high bias will then result in a large band around $\mathbf{\tilde{y}}$ in which $y_i$ will exist. The variance term is a measure of how much the $\mathbf{\tilde{y}}$ varies around its own mean. In other terms a high variance model will experience large variance in the predicted values $\mathbf{\tilde{y}}$ from one test set to another. Or as a result of this a large variance in score if you will. Now these two terms are in a sense opposites, it is likely that a model with high bias will have low variance and vice versa. This is something we as designers of the model get to tune, if we value a lower bias more than we value a low variance or vice versa, we can choose a model accordingly
%
%
%
%
%
%
%
%
%
%
%
%
%
%



























% APPENDIX

% Acknowledgements should go at the end, before appendices and references
% \acks{}
%
% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.
%
% \newpage
% \appendix
% \phantomsection%
% \addcontentsline{toc}{section}{Appendix}
% \section*{Appendix}
% \label{app:appendix}
%
% \phantomsection%
% \addcontentsline{toc}{subsection}{SVD}
% \subsection*{SVD}
% \label{app:svd}
% svd
%
%
%
%
%
% adding these two we get
% $$
%     \frac{\partial \left[\frac{1}{n}\vert\vert \mathbf{y}-\mathbf{X}\mathbf{\beta}\vert\vert_2^2+\lambda\vert\vert \mathbf{\beta}\vert\vert_2^2\right]}{\partial{\mathbf{\beta}}} = 0 = \frac{2}{n}\left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)\mathbf{\beta}-\frac{2}{n}\mathbf{X}^T\mathbf{y} \implies \beta = \left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{y}
% $$
%
%
%
% The minimization of ridge addition alone is
% $${\displaystyle \min_{\mathbf{\beta}\in
% {\mathbb{R}}^{p}}}\frac{1}{n} \lambda \vert\vert \mathbf{\beta}\vert\vert_2^2 = \frac{2}{n} \lambda \mathbf{\beta}$$
%
%
%
%
% We know that 
% $$
% {\displaystyle \min_{\mathbf{\beta}\in
% {\mathbb{R}}^{p}}}\frac{1}{n} \vert\vert\mathbf{y}- \mathbf{X}\mathbf{\beta}\vert\vert_2^2 = \frac{2}{n} \mathbf{X}^T\mathbf{X}\mathbf{\beta}-\frac{2}{n}\mathbf{X}^T\mathbf{y}
% $$
%
%
%
%
% $$
% {\displaystyle \min_{\mathbf{\beta}\in
% {\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\vert\vert \mathbf{y}-\mathbf{X}\mathbf{\beta}\vert\vert_2^2,
% $$
%
%
%
%
% $$
% {\displaystyle \min_{\mathbf{\beta}\in
% {\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \mathbf{y}-\mathbf{X}\mathbf{\beta}\vert\vert_2^2+\lambda\vert\vert \mathbf{\beta}\vert\vert_2^2
% $$
%
%
%
%
% $$
% \hat{\mathbf{\beta}}_{\mathrm{OLS}} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y},
% $$
%
%
%
%
% $$
% \hat{\mathbf{\beta}}_{\mathrm{Ridge}} = \left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{y},
% $$
%
%
%
% Use the singular value decomposition of an $n\times p$ matrix $\mathbf{X}$ (our design matrix)
% $$
% \mathbf{X}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T,
% $$
%
%
% where $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices of dimensions
% $n\times n$ and $p\times p$, respectively, and $\mathbf{\Sigma}$ is an
% $n\times p$ matrix which contains the ingular values only. This material was discussed during the lectures of week 35.
%
% Show that you can write the 
% OLS solutions in terms of the eigenvectors (the columns) of the orthogonal matrix  $\mathbf{U}$ as
%
%
% $$
% \tilde{\mathbf{y}}_{\mathrm{OLS}}=\mathbf{X}\mathbf{\beta}  = \sum_{j=0}^{p-1}\mathbf{u}_j\mathbf{u}_j^T\mathbf{y}.
% $$
%
%
% $$
% \mathbf{X}^T\mathbf{X} = \mathbf{V}\mathbf{\Sigma}^T\mathbf{U}^T\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T = \mathbf{V}\mathbf{\Sigma}^T\mathbf{\Sigma}\mathbf{V}^T = \mathbf{V}\mathbf{\Sigma}^2\mathbf{V}^T
% $$
%
% $$
% \tilde{\mathbf{y}}_{\mathrm{OLS}} = \mathbf{X}\mathbf{\beta} = \mathbf{X} \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y}
% \\ = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T \left( \mathbf{V}\mathbf{\Sigma}^2\mathbf{V}^T \right)^{-1} \mathbf{V}\mathbf{\Sigma}^T\mathbf{U}^T \mathbf{y}
% \\ = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T {\mathbf{V}^T}^{-1} {\mathbf{\Sigma}^2}^{-1} \mathbf{V}^{-1}  \mathbf{V}\mathbf{\Sigma}^T\mathbf{U}^T \mathbf{y}
% $$
% using the orthogonality of $\mathbf{V}$ and $\mathbf{U}$ we get. Multiplying with $\mathbf{\Sigma}$ removes columns from $\mathbf{U}$ with eigenvalues equal to zero.
% $$
% \tilde{\mathbf{y}}_{\mathrm{OLS}} = \mathbf{U}\mathbf{U}^T \mathbf{y}= \sum_{j=0}^{p-1}\mathbf{u}_j\mathbf{u}_j^T\mathbf{y}
% $$
%
%
% $$
% \mathbf{y} = f(\mathbf{x})+\mathbf{\varepsilon}
% $$
%
% $$
% \mathbf{\tilde{y}} = \mathbf{X}\mathbf{\beta}.
% $$
%
% $$
% \mathbb{E}(y_i)  =\sum_{j}x_{ij} \beta_j=\mathbf{X}_{i, \ast} \, \mathbf{\beta}
% $$
%
% $$
% {Var}(y_i)  = \sigma^2
% $$
% $$
% {Var}(\mathbf{\hat{\beta}}) = \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1}.
% $$
% \phantomsection%
% \addcontentsline{toc}{subsection}{Confidence Interval}
% \subsection*{Math Behind the Confidence Interval section}
% \label{app:confidenceInterval}
% We can assume that $y$ follows some function $f$ with some noise $\epsilon$
% $$
% \mathbf{y} = f(\mathbf{x}) + \epsilon
% $$
% $$
% \mathbb{E}[\mathbf{y}] = \mathbb{E}[f(\mathbf{x} )+ \epsilon] = \mathbb{E}[f(\mathbf{x})] + \mathbb{E}[\epsilon_i]
% $$
% The expected value of $\epsilon_i$ is $0$, $f(x)$ is a non-stochastic variable and is approimated by $\mathbf{X}\mathbf{\beta}$
% $$
% \mathbb{E}[y_i] = \mathbf{X_{i,*}}\mathbf{\beta}
% $$
% The variance is defined as
% \begin{align*}
% Var(y_i) &= \mathbb{E}\big[(y_i - \mathbb{E}[y_i])^2\big] = \mathbb{E}\left[y_i^2 - 2y_i\mathbb{E}[y_i] + \mathbb{E}[y_i]^2\right] = \mathbb{E}[y_i^2] - \mathbb{E}[y_i]^2\\
% Var(y_i) &= \mathbb{E}\big[\big(\mathbf{X}_{i,*}\mathbf{\beta}\big)^2 + 2\epsilon \mathbf{X}_{i,*}\mathbf{\beta} + \epsilon^2 \big] - (\mathbf{X}_{i,*}\mathbf{\beta})^2\\
% Var(y_i) &= (\mathbf{X}_{i,*}\mathbf{\beta})^2 + 2\mathbb{E}[\epsilon]\mathbf{X}_{i,*}\mathbf{\beta} + \mathbb{E}[\epsilon^2] - (\mathbf{X}_{i,*}\mathbf{\beta})^2\\
% Var(y_i) &= \mathbb{E}[\epsilon^2] = \sigma^2
% \end{align*}
% for the expected value of $\mathbf{\beta}$ we can insert the definition of $\mathbf{\beta}$ from earlier
%
%
% $$
% \mathbb{E}[\mathbf{\hat{\beta}}] = \mathbb{E}[ (\mathbf{X}^{\top} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbb{E}[ \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1} \mathbf{X}^{T}\mathbf{X}\mathbf{\beta}=\mathbf{\beta}
% $$
%
%
% \begin{align*}
% Var(\mathbf{\hat{\beta}}) & = \mathbb{E} \{ [\mathbf{\beta} - \mathbb{E}(\mathbf{\beta})] [\mathbf{\beta} - \mathbb{E}(\mathbf{\beta})]^{T} \}
% \\
% & = \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{y} - \mathbf{\beta}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{y} - \mathbf{\beta}]^{T} \}
% \\
% & = (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \mathbb{E} \{ \mathbf{y} \, \mathbf{y}^{T} \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \mathbf{\beta} \, \mathbf{\beta}^{T}
% \\
% & = (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \{ \mathbf{X} \, \mathbf{\beta} \, \mathbf{\beta}^{T} \,  \mathbf{X}^{T} + \sigma^2 \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \mathbf{\beta} \, \mathbf{\beta}^{T}
% \\
% & = \mathbf{\beta} \, \mathbf{\beta}^{T}  + \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \mathbf{\beta} \, \mathbf{\beta}^{T}
% \, \, \, = \, \, \, \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1},
% \end{align*}


% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:



















\vskip 0.2in
\bibliography{report}
% \bibliographystyle{apalike}
\bibliographystyle{plain}
\phantomsection%
\addcontentsline{toc}{section}{Bibliography}
\end{document}








