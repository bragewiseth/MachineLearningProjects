\documentclass[twoside,11pt]{report}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
% \usepackage[utf8]{inputenc}%
% \usepackage{tikz}
% \usepackage{cfr-lm}%
\usepackage[T1]{fontenc}%
\usepackage{physics}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage[margin=3cm]{geometry}
% \usepackage{changepage}
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{lettrine}
% \usepackage{fontawesome}
\usemintedstyle{perldoc}
\usepackage{hyperref}
\hypersetup{colorlinks=false, pdfborder={0 0 0}}
\usepackage{fancyhdr}



\newtcbox{\codebox}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1pt, boxsep=0pt,left=2pt,right=2pt,top=1pt,bottom=.5pt}
\newtcbox{\deloppg}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-2pt]{0pt}{0pt}},boxrule=0pt, boxsep=0pt,left=.49\linewidth,right=.49\linewidth,top=4pt,bottom=3pt}


\newcommand\blfootnote[1]{ \begingroup \renewcommand\thefootnote{}\footnote{#1} \addtocounter{footnote}{-1} \endgroup }
% \definecolor{antwhite}{HTML}{323333}
\newcommand{\code}[3][]{\codebox{\mintinline[#1]{#2}{#3}}}



% \setmainfont{FreeSans}
% \setmainfont{SF Pro Display}
% \setmainfont{IBM Plex Sans}
% \setmainfont{TeX Gyre Heros}
% \setmainfont{Inter}
% \setmainfont{Iosevka Quasi}

% \setmonofont{Iosevka Custom Extended}
% \setmonofont{JetBrainsMono Nerd Font}
\setmonofont[Scale=MatchLowercase]{SF Mono}





% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{https://github.com/bragewiseth/MachineLearningProjects}

% Short headings should be running head and authors last names

\ShortHeadings{\url{https://github.com/bragewiseth/MachineLearningProjects}}{\url{https://github.com/bragewiseth/MachineLearningProjects}}
\firstpageno{1}



\title{{\huge Project 1}}
\author{\name Eirik \email eirk@ifi.uio.no \\
       \name Felix  \email felix@ifi.uio.no \\
       \name Brage W. \email bragewi@ifi.uio.no}
\date{\today}											% Date
\makeatletter






% \date{\today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{uio.jpg}\\[1.0 cm]	% University Logo
    \textsc{\LARGE University of Oslo}\\[2.0 cm]	% University Name
	\textsc{\Large FYS-STK3155}\\[0.5 cm]				% Course Code
	\rule{\linewidth}{0.2 mm} \\[0.4 cm]
	{ \huge \bfseries \@title}\\
	\rule{\linewidth}{0.2 mm} \\[1.5 cm]

	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \normalsize
			Eirik\\
            Felix\\
            Brage Wiseth\\
			\end{flushleft}
			\end{minipage}~
			\begin{minipage}{0.4\textwidth}
			\begin{flushright} \normalsize
        \textsc{
			    eirik@ifi.uio.no\\
          felix@ifi.uio.no\\
          bragewi@ifi.uio.no\\
        }
		\end{flushright}
        
	\end{minipage}\\[2 cm]
	\@date\\
    \vspace*{25mm}
    \urlstyle{rm}
    \textsc{\url{https://github.com/bragewiseth/MachineLearningProjects}}
	
	
    
    
    
    
	
\end{titlepage}
\nocite{*}
% \maketitle
\newpage
\tableofcontents
\newpage

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\lettrine{T}{}his paper describes the mixtures-of-trees model, a probabilistic 
model for discrete multidimensional domains.  Mixtures-of-trees 
generalize the probabilistic trees of 
in a different and complementary direction to that of Bayesian networks.
We present efficient algorithms for learning mixtures-of-trees 
models in maximum likelihood and Bayesian frameworks. 
We also discuss additional efficiencies that can be
obtained when data are ``sparse,'' and we present data 
structures and algorithms that exploit such sparseness.
Experimental results demonstrate the performance of the 
model for both density estimation and classification. 
We also discuss the sense in which tree-based classifiers
perform an implicit form of feature selection, and demonstrate
a resulting insensitivity to irrelevant attributes.
\end{abstract}

\begin{keywords}
    Linear Regression, Fitting Polynomials, Scaling, Bias-Variance 
\end{keywords}

\section{Introduction}

Franke function bla bla bla blablbisdjaidh
sdhkadhjsiajdsdsah
% \begin{figure*}[h]
%     \centering
%     \resizebox*{0.9\linewidth}{!}{\input{Figure_1.pgf}}
% \end{figure*}

% Acknowledgements should go at the end, before appendices and references



\pagebreak

\section{Ordinary Least Squares}
\section{Ridge}
\section{Lasso}
\subsection{Discussion on Scaling}


Unscaled sample design matrix fitting one-dimensional polynomial of degree 5
\begin{table}[h]
\tt
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{llllll}
    1. &     0.  &    0.  &    0.   &   0.   &   0.     \\
    1. &     0.25 &   0.0625 & 0.01562& 0.00391& 0.00098\\
    1.    &  0.5     &0.25 &   0.125 &  0.0625 & 0.03125\\
    1.   &   0.75  &  0.5625 & 0.42188 &0.31641& 0.2373 \\
    1.  &    1.   &   1.  &    1.  &    1.    &  1.
\end{tabular}%
}
\end{table}\\
Scaled sample design matrix fitting one-dimensional polynomial of degree 5
\begin{table}[h]
\tt
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{llllll}
     0. &     -1.41421& -1.0171&  -0.83189& -0.728 &  -0.66226\\
     0.  &    -0.70711& -0.84758& -0.7903 & -0.71772& -0.65971\\
     0.  &     0.    &  -0.33903& -0.49913& -0.56348& -0.58075\\
     0.  &     0.70711&  0.50855&  0.29116 & 0.10488& -0.0433 \\
     0.  &     1.41421&  1.69516 & 1.83016 & 1.90431&  1.94603
\end{tabular}%
}
\end{table}
\begin{table}[h]
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{ll}
MSE for OLS on unscaled data:    &   \texttt{0.010349396022903145} \\
MSE for OLS on scaled data:      &   \texttt{0.010349396024145656} \\
MSE for Ridge on unscaled data:  &   \texttt{0.02106077418650843} \\
MSE for Ridge on scaled data:    &   \texttt{0.01782525371566323}
\end{tabular}%
}
\end{table}
\begin{center}
    \scriptsize
    the code for genertating this output \footnote{\url{https://github.com/bragewiseth/MachineLearningProjects/blob/main/project1/src/linearRegression.py}\cite{MachineLearningProjects_2023}}
\end{center}

First $x_{\text{unscaled}}$ and $x_{\text{scaled}}$ is not that different, the original data was close to zero-centerd and not that spread out, which means that initaially by just looking at the data scaling is not that nececary. When we add the polynomial terms we can now see that some of the entries of $X_{\text{unscaled}}$ get really small as an exsample $0.1^5 = 0.00001$ this makes the collums of $X_{\text{unscaled}}$ live in their own order of magnitude and scaling should be considerd to bring them back to the same ish order of magnitude. The act of not scaling results in $\boldsymbol{\beta}$ spanning from $-50$ to $48$ while scaling gives a smaller span from $-10$ to $13$\footnote{We would imagine that keeping everything in the same $0$ order of magnitude is something that the computer likes, perhaps improving performance}. Now this alone may not justify why we should scale this dataset, as scaled and unscaled OLS yields the same MSE. However when doing ridge regression the cost function is directly dependent on the magnitude of $\beta_i$. Now with each $\beta_i$ variying alot, some are getting more penaliesed than others. As we can see the MSE for unscaled data is much higher than for scaled data in the ridge case.
This leads us to conclude that we should scale the data, making it easier to tweek $\lambda$ and giving us nicer numbers to work with.


\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{report}
% \bibliographystyle{apalike}
\bibliographystyle{plain}
\addcontentsline{toc}{section}{Bibliography}
\end{document}







% \fontsize{9.5}{11.4}
% \selectfont

% \begin{figure}[!htb]
%     \centering
%     \resizebox{0.28\textwidth}{!}{\input{.pdf_tex}}
% \end{figure}

% \begin{minipage}[t]{.46\linewidth}
    % \includegraphics[width=78mm]{.pdf}
% \end{minipage}

% \begin{tcolorbox}[colframe=white]
%     \textbf{Oppgave I}
% \end{tcolorbox}
