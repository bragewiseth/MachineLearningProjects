\documentclass[twoside,11pt]{report}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
% \usepackage[utf8]{inputenc}%
% \usepackage{tikz}
% \usepackage{cfr-lm}%
\usepackage[T1]{fontenc}%
\usepackage{physics}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage[margin=3cm]{geometry}
% \usepackage{changepage}
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{lettrine}
% \usepackage{fontawesome}
\usemintedstyle{perldoc}
\usepackage{hyperref}
\hypersetup{colorlinks=false, pdfborder={0 0 0}}
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{adjustbox}



\newtcbox{\codebox}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1pt, boxsep=0pt,left=2pt,right=2pt,top=1pt,bottom=.5pt}
\newtcbox{\deloppg}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-2pt]{0pt}{0pt}},boxrule=0pt, boxsep=0pt,left=.49\linewidth,right=.49\linewidth,top=4pt,bottom=3pt}


\newcommand\blfootnote[1]{ \begingroup \renewcommand\thefootnote{}\footnote{#1} \addtocounter{footnote}{-1} \endgroup }
% \definecolor{antwhite}{HTML}{323333}
\newcommand{\code}[3][]{\codebox{\mintinline[#1]{#2}{#3}}}



% \setmainfont{FreeSans}
% \setmainfont{SF Pro Display}
% \setmainfont{IBM Plex Sans}
% \setmainfont{TeX Gyre Heros}
% \setmainfont{Inter}
% \setmainfont{Iosevka Quasi}

% \setmonofont{Iosevka Custom Extended}
% \setmonofont{JetBrainsMono Nerd Font}
% \setmonofont[Scale=MatchLowercase]{SF-Mono-Medium}





% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{https://github.com/bragewiseth/MachineLearningProjects}

% Short headings should be running head and authors last names

\ShortHeadings{\url{https://github.com/bragewiseth/MachineLearningProjects}}{\url{https://github.com/bragewiseth/MachineLearningProjects}}
\firstpageno{1}



\title{{\huge Project 1}}
\author{\name Eirik \email eirk@ifi.uio.no \\
       \name Felix  \email felixch@ifi.uio.no \\
       \name Brage W. \email bragewi@ifi.uio.no}
\date{\today}											% Date
\makeatletter






% \date{\today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{uio.jpg}\\[1.0 cm]	% University Logo
    \textsc{\LARGE University of Oslo}\\[2.0 cm]	% University Name
	\textsc{\Large FYS-STK3155}\\[0.5 cm]				% Course Code
	\rule{\linewidth}{0.2 mm} \\[0.4 cm]
	{ \huge \bfseries \@title}\\
	\rule{\linewidth}{0.2 mm} \\[1.5 cm]

	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \normalsize
			Eirik\\
            Felix\\
            Brage Wiseth\\
			\end{flushleft}
			\end{minipage}~
			\begin{minipage}{0.4\textwidth}
			\begin{flushright} \normalsize
        \textsc{
			    eirik@ifi.uio.no\\
          felix@ifi.uio.no\\
          bragewi@ifi.uio.no\\
        }
		\end{flushright}
        
	\end{minipage}\\[2 cm]
	\@date\\
    \vspace*{25mm}
    \urlstyle{rm}
    \textsc{\url{https://github.com/bragewiseth/MachineLearningProjects}}
	
	
    
    
    
    
	
\end{titlepage}
\nocite{*}
% \maketitle
\newpage
\tableofcontents
\newpage

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\lettrine{I}{}n this project, we explored the use of different methods for solving linear regres-
sion on topological data. We emplyed OLS, ridge and lasso methods for linear regression on
simulated height data using \emph{Franke's Function}, Where the goal was to fit polynomials to minimize the
mean square error. As well as real topological data with the same goal.
\end{abstract}
\begin{keywords}
    Linear Regression, Fitting Polynomials, Scaling, Bias-Variance 
\end{keywords}

\section{Introduction}

\begin{wrapfigure}{l}{0.3\textwidth}
    \caption{\small Franke's Function}
    \label{frankfunc}
    \begin{center}
    \begin{adjustbox}{clip,trim=4cm 3.4cm 3cm 4.8cm, max width=0.3\textwidth}
    \input{Frank_2.pgf}
    \end{adjustbox}
    \end{center}
\end{wrapfigure}
To get a smooth start on this project we begin to construct some fake topological data. This can be constructed with
\emph{Franke's Function}. This gives us a nice dataset to start linear regression. 
A natrual start would be to use ordinary least squares to try to fit a polynomial to some degree. This is done by
constructing a design matrix $X$ with the polynomial terms and then solving the linear regression equation
\begin{equation}
    \boldsymbol{\beta} = (X^T X)^{-1} X^T \boldsymbol{y}
\end{equation}
where $\boldsymbol{\beta}$ is the coefficients of the polynomial and $\boldsymbol{y}$ is the data.
This is a nice start, but we can do better. We can add a penalty term to the cost function, this is called ridge regression.
\begin{equation}
    \boldsymbol{\beta} = (X^T X + \lambda I)^{-1} X^T \boldsymbol{y}
\end{equation}

\begin{wrapfigure}{r}{0.3\textwidth}
\begin{center}
    \begin{adjustbox}{clip,trim=4cm 3.4cm 3cm 4.8cm, max width=0.3\textwidth}
    \input{Frank_1.pgf}
    \end{adjustbox}
    \caption{\small Franke's Function with noise}
    \label{frankfuncnoise}
    \end{center}
\end{wrapfigure}



where $\lambda$ is a hyperparameter that we can tune to get a better fit. This is a nice start, but we can do better. We can add a penalty term to the cost function, this is called ridge regression.
\begin{equation}
    \boldsymbol{\beta} = (X^T X + \lambda I)^{-1} X^T \boldsymbol{y}
\end{equation}
where $\lambda$ is a hyperparameter that we can tune to get a better fit. This is a nice start, but we can do better. We can add a penalty term to the cost function, this is called ridge regression.
\begin{equation}
    \boldsymbol{\beta} = (X^T X + \lambda I)^{-1} X^T \boldsymbol{y}
\end{equation}
where $\lambda$ is a hyperparameter that we can tune to get a better fit. This is a nice start, but we can do better. We can add a penalty term to the cost function, this is called ridge regression.


\subsection{Structure of the report}



\section{Discussion on Scaling}


\begin{table}[!h]
\caption{Unscaled sample design matrix fitting one-dimensional polynomial of degree 5}
\tt
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{llllll}
    1. &     0.  &    0.  &    0.   &   0.   &   0.     \\
    1. &     0.25 &   0.0625 & 0.01562& 0.00391& 0.00098\\
    1.    &  0.5     &0.25 &   0.125 &  0.0625 & 0.03125\\
    1.   &   0.75  &  0.5625 & 0.42188 &0.31641& 0.2373 \\
    1.  &    1.   &   1.  &    1.  &    1.    &  1.
\end{tabular}%
}
\end{table}
\begin{table}[!h]
\caption{Scaled sample design matrix fitting one-dimensional polynomial of degree 5}
\tt
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{llllll}
     0. &     -1.41421& -1.0171&  -0.83189& -0.728 &  -0.66226\\
     0.  &    -0.70711& -0.84758& -0.7903 & -0.71772& -0.65971\\
     0.  &     0.    &  -0.33903& -0.49913& -0.56348& -0.58075\\
     0.  &     0.70711&  0.50855&  0.29116 & 0.10488& -0.0433 \\
     0.  &     1.41421&  1.69516 & 1.83016 & 1.90431&  1.94603
\end{tabular}%
}
\end{table}
\begin{table}[h]
\centering
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{ll}
MSE for OLS on unscaled data:    &   \texttt{0.010349396022903145} \\
MSE for OLS on scaled data:      &   \texttt{0.010349396024145656} \\
MSE for Ridge on unscaled data:  &   \texttt{0.02106077418650843} \\
MSE for Ridge on scaled data:    &   \texttt{0.01782525371566323}
\end{tabular}%
}\\
\scriptsize
the code for genertating this output \footnote{\url{https://github.com/bragewiseth/MachineLearningProjects/blob/main/project1/src/linearRegression.py}\cite{MachineLearningProjects_2023}}
\end{table}
First $x_{\text{unscaled}}$ and $x_{\text{scaled}}$ is not that different, the original data was close to zero-centerd and not that spread out, which means that initaially by just looking at the data scaling is not that nececary. When we add the polynomial terms we can now see that some of the entries of $X_{\text{unscaled}}$ get really small as an exsample $0.1^5 = 0.00001$ this makes the collums of $X_{\text{unscaled}}$ live in their own order of magnitude and scaling should be considerd to bring them back to the same ish order of magnitude \footnote{We would imagine that keeping everything in the same $0$ order of magnitude is something that the computer likes, perhaps improving performance}. The act of not scaling results in $\boldsymbol{\beta}$ spanning from $-50$ to $48$ while scaling gives a smaller span from $-10$ to $13$. Now this alone may not justify why we should scale this dataset, as scaled and unscaled OLS yields the same MSE. However when doing ridge regression the cost function is directly dependent on the magnitude of $\beta_i$. Now with each $\beta_i$ variying alot, some are getting more penaliesed than others. As we can see the MSE for unscaled data is much higher than for scaled data in the ridge case.
This leads us to conclude that we should scale the data, making it easier to tweek $\lambda$ and giving us nicer numbers to work with.
\subsection{Analysis}
\begin{figure}[!h]
    \begin{center}
    \resizebox*{0.9\linewidth}{!}{\input{Figure_2.pgf}}
    \caption{Franke's Function}
    \label{ols}
    \end{center}
\end{figure}
\begin{figure}[!h]
    \begin{center}
    \resizebox*{0.7\linewidth}{!}{\input{beta-modelcomplexity.pgf}}
    \caption{Franke's Function}
    \label{}
    \footnote{https}
    \end{center}
\end{figure}

\section{Ordinary Least Squares}
As we increase the order of the polynomial fit we see a corresponding decrease in MSE and increase in R2.
\section{Ridge}
\section{Lasso}
\section{Paper \& Pencil}
The assumption we have made is that there exists a continuous function f (x)
and a normal distributed error ε ∼ N (0, σ2) which describes our data
y = f (x) + ε
We then approximate this function f (x) with our model ˜y from the solution
of the linear regression equations (ordinary least squares OLS), that is our
function f is approximated by ˜y where we minimized (y − ˜y)2, with
˜y = Xβ.
The matrix X is the so-called design or feature matrix.
Show that the expectation value of y for a given element i
E(yi) = ∑
j
xij βj = Xi,∗ β,
and that its variance is
Var(yi) = σ2.
Hence, yi ∼ N (Xi,∗ β, σ2), that is y follows a normal distribution with mean
value Xβ and variance σ2.
With the OLS expressions for the optimal parameters ˆβ show that
E( ˆβ) = β.
Show finally that the variance of β is
Var( ˆβ) = σ2 (XT X)−1.
We can use the last expression when we define a so-called confidence interval
for the parameters β. A given parameter βj is given by the diagonal matrix
element of the above matrix.


We can assume that $y$ follows some function $f$ with some noise $\epsilon$
$$
\mathbf{y} = f(\mathbf{x}) + \epsilon
$$
$$
\mathbb{E}[\mathbf{y}] = \mathbb{E}[f(\mathbf{x} )+ \epsilon] = \mathbb{E}[f(\mathbf{x})] + \mathbb{E}[\epsilon_i]
$$
The expected value of $\epsilon_i$ is $0$, $f(x)$ is a non-stochastic variable and is approimated by $\boldsymbol{X}\boldsymbol{\beta}$
$$
\mathbb{E}[y_i] = \boldsymbol{X_{i,*}}\boldsymbol{\beta}
$$
The variance is defined as
\begin{align*}
Var(y_i) &= \mathbb{E}\big[(y_i - \mathbb{E}[y_i])^2\big] = \mathbb{E}\left[y_i^2 - 2y_i\mathbb{E}[y_i] + \mathbb{E}[y_i]^2\right] = \mathbb{E}[y_i^2] - \mathbb{E}[y_i]^2\\
Var(y_i) &= \mathbb{E}\big[\big(\boldsymbol{X}_{i,*}\boldsymbol{\beta}\big)^2 + 2\epsilon \boldsymbol{X}_{i,*}\boldsymbol{\beta} + \epsilon^2 \big] - (\boldsymbol{X}_{i,*}\boldsymbol{\beta})^2\\
Var(y_i) &= (\boldsymbol{X}_{i,*}\boldsymbol{\beta})^2 + 2\mathbb{E}[\epsilon]\boldsymbol{X}_{i,*}\boldsymbol{\beta} + \mathbb{E}[\epsilon^2] - (\boldsymbol{X}_{i,*}\boldsymbol{\beta})^2\\
Var(y_i) &= \mathbb{E}[\epsilon^2] = \sigma^2
\end{align*}
for the expected value of $\boldsymbol{\beta}$ we can insert the definition of $\boldsymbol{\beta}$ from earlier
$$
\mathbb{E}[\boldsymbol{\hat{\beta}}] = \mathbb{E}[ (\mathbf{X}^{\top} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbb{E}[ \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1} \mathbf{X}^{T}\mathbf{X}\boldsymbol{\beta}=\boldsymbol{\beta}.
$$
\begin{align*}
Var(\boldsymbol{\hat{\beta}}) & = \mathbb{E} \{ [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})] [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})]^{T} \}
\\
& = \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{y} - \boldsymbol{\beta}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{y} - \boldsymbol{\beta}]^{T} \}
\\
& = (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \mathbb{E} \{ \mathbf{y} \, \mathbf{y}^{T} \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\\
& = (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \{ \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \,  \mathbf{X}^{T} + \sigma^2 \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\\
& = \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}  + \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\, \, \, = \, \, \, \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1},
\end{align*}






\section{Bias \& Variance}

\begin{align*}
    \text{Var}(\boldsymbol{\tilde{y}}) &= \mathbb{E}\left[\left(\tilde{\boldsymbol{y}}-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right]\right)^2\right] = \mathbb{E}[\boldsymbol{\tilde{y}}^2] - \mathbb{E}[\boldsymbol{\tilde{y}}]^2\\
    \mathbb{E}[\boldsymbol{\tilde{y}}^2] &= \mathbb{E}[\boldsymbol{\tilde{y}}]^2 + \text{Var}(\boldsymbol{\tilde{y}})
\end{align*}


\begin{align*}
\mathbb{E}[\boldsymbol{y}^2] & = \mathbb{E}[\boldsymbol{f} + \boldsymbol{\epsilon}]^2 = \mathbb{E}[\boldsymbol{f}^2 + 2\boldsymbol{f}\boldsymbol{\epsilon} + \boldsymbol{\epsilon}^2]\\
& = \mathbb{E}[\boldsymbol{f}^2] + 2\mathbb{E}[\boldsymbol{f}\boldsymbol{\epsilon}] + \mathbb{E}[\boldsymbol{\epsilon}^2]\\
& = \mathbb{E}[\boldsymbol{f}^2] + 2\mathbb{E}[\boldsymbol{f}]\mathbb{E}[\boldsymbol{\epsilon}] + \mathbb{E}[\boldsymbol{\epsilon}^2]\\
& = \mathbb{E}[\boldsymbol{f}^2]  + \sigma^2
\end{align*}


\begin{align*}
\mathbb{E}[\boldsymbol{y\tilde{y}}] & = \mathbb{E}[\boldsymbol{f\tilde{y}} + \boldsymbol{\epsilon\tilde{y}}]\\
& = \mathbb{E}[\boldsymbol{f\tilde{y}}] + \mathbb{E}[\boldsymbol{\epsilon\tilde{y}}]\\
& = \mathbb{E}[\boldsymbol{f\tilde{y}}] + \mathbb{E}[\boldsymbol{\epsilon}]\mathbb{E}[\boldsymbol{\tilde{y}}]\\
& = \boldsymbol{f}\mathbb{E}[\boldsymbol{\tilde{y}}]
\end{align*}


\begin{align*}
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right] & = \mathbb{E}[\boldsymbol{y}^2] - 2\mathbb{E}[\boldsymbol{y\tilde{y}}] + \mathbb{E}[\boldsymbol{\tilde{y}}^2]\\
& = \boldsymbol{f}^2 + \sigma^2 - 2\boldsymbol{f}\mathbb{E}[\boldsymbol{\tilde{y}}] + \mathbb{E}[\boldsymbol{\tilde{y}}^2]\\
& = \boldsymbol{f}^2 + \sigma^2 - 2\boldsymbol{f}\mathbb{E}[\boldsymbol{\tilde{y}}] + \mathbb{E}[\boldsymbol{\tilde{y}}]^2 + \text{Var}(\boldsymbol{\tilde{y}})\\
& = (\boldsymbol{f} - \mathbb{E}[\boldsymbol{\tilde{y}}])^2 + \text{Var}(\boldsymbol{\tilde{y}}) + \sigma^2\\
\end{align*}


\begin{align*}
\mathrm{Bias}[\tilde{y}]& =\mathbb{E}\left[\left(\boldsymbol{y}-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right]\right)^2\right] = \mathbb{E}[\boldsymbol{y}^2] - 2\mathbb{E}[\boldsymbol{y}]\mathbb{E}[\boldsymbol{\tilde{y}}] + \mathbb{E}[\boldsymbol{\tilde{y}}^2]\\
& = \boldsymbol{f}^2 + \sigma^2 - 2\boldsymbol{f}\mathbb{E}[\boldsymbol{\tilde{y}}] + \mathbb{E}[\boldsymbol{\tilde{y}}]^2\\
& = (\boldsymbol{f} - \mathbb{E}[\boldsymbol{\tilde{y}}])^2 + \sigma^2\\
\end{align*}


To find $\mathbb{E}[\boldsymbol{\tilde{y}}]$ we can use bootstrap to get different samples for $\boldsymbol{\tilde{y}}$ and then take the mean of that distribution.
The term \emph{variance} refers to how spread out our observations are. We can for our case think of the observations $\tilde{y}_i$
as either individual predictions, or the mean of predictions from one test set.\\
In a sense the bias resembles the mathematical defninition for variance, We can see that the bias term is a distance measure of $\boldsymbol{y}$ from the expected value of $\boldsymbol{\tilde{y}}$. If we take the individual observations approach this can be tough of as A high bias will then result in a large band around $\boldsymbol{\tilde{y}}$ in which $y_i$ will exist. The variance term is a measure of how much the $\boldsymbol{\tilde{y}}$ varies around its own mean. In other terms a high variance model will experience large variance in the predicted values $\boldsymbol{\tilde{y}}$ from one test set to another. Or as a result of this a large variance in score if you will. Now these two terms are in a sense opposites, it is likely that a model with high bias will have low variance and vice versa. This is something we as designers of the model get to tune, if we value a lower bias more than we value a low variance or vice versa, we can choose a model acordingly

% Acknowledgements should go at the end, before appendices and references
\acks{}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:


\vskip 0.2in
\bibliography{report}
% \bibliographystyle{apalike}
\bibliographystyle{plain}
\addcontentsline{toc}{section}{Bibliography}
\end{document}







% \fontsize{9.5}{11.4}
% \selectfont

% \begin{figure}[!htb]
%     \centering
%     \resizebox{0.28\textwidth}{!}{\input{.pdf_tex}}
% \end{figure}

% \begin{minipage}[t]{.46\linewidth}
    % \includegraphics[width=78mm]{.pdf}
% \end{minipage}

% \begin{tcolorbox}[colframe=white]
%     \textbf{Oppgave I}
% \end{tcolorbox}
