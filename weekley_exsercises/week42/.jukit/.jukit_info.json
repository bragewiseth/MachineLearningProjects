{"cmd": "r\"\"\"°°°\n# Overarching aims of the exercises this week\n\nThe aim of the exercises this week is to get started with implementing\ngradient methods of relevance for project 2. The exercise this week is a simple\ncontinuation from the  previous week with the addition of automatic differentation.\nEverything you develop here will be used in project 2. \n\nIn order to get started, we will now replace in our standard ordinary\nleast squares (OLS) and Ridge regression codes (from project 1) the\nmatrix inversion algorithm with our own gradient descent (GD) and SGD\ncodes.  You can use the Franke function or the terrain data from\nproject 1. **However, we recommend using a simpler function like**\n$f(x)=a_0+a_1x+a_2x^2$ or higher-order one-dimensional polynomials.\nYou can obviously test your final codes against for example the Franke\nfunction. Automatic differentiation will be discussed next week.\n\nYou should include in your analysis of the GD and SGD codes the following elements\n1. A plain gradient descent with a fixed learning rate (you will need to tune it) using automatic differentiation. Compare this with the analytical expression of the gradients you obtained last week. Feel free to use **Autograd** as Python package or **JAX**. You can use the examples form last week.\n\n2. Add momentum to the plain GD code and compare convergence with a fixed learning rate (you may need to tune the learning rate). Compare this with the analytical expression of the gradients you obtained last week.\n\n3. Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. Use a tunable learning rate as discussed in the lectures from week 39. Discuss the results as functions of the various parameters (size of batches, number of epochs etc)\n\n4. Implement the Adagrad method in order to tune the learning rate. Do this with and without momentum for plain gradient descent and SGD using automatic differentiation..\n\n5. Add RMSprop and Adam to your library of methods for tuning the learning rate. Again using automatic differentiation.\n\nThe lecture notes from weeks 39 and 40 contain more information and code examples. Feel free to use these examples.\n\nWe recommend reading chapter 8 on optimization from the textbook of [Goodfellow, Bengio and Courville](https://www.deeplearningbook.org/). This chapter contains many useful insights and discussions on the optimization part of machine learning.\n°°°\"\"\"", "import_complete": 0, "terminal": "nvimterm", "cmd_opts": " --cell_id=Pkv3ic6gIQ", "outhist_cell": "Pkv3ic6gIQ", "is_md": 1}