{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd99277-6f6a-4d3d-a441-7e2f3a70f7a5",
   "metadata": {},
   "source": [
    "# Week 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743df43a-7fd9-4949-ba97-d99f8dd78c40",
   "metadata": {},
   "source": [
    "### OLS gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951dc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56236c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinRegClass():\n",
    "\n",
    "    def __init__(self, bias=-1, dim_hidden=0):\n",
    "        self.bias=bias\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, t_train, X_val=None, t_val=None, eta = 0.1, epochs=10, tol=0.001):\n",
    "\n",
    "        if self.bias:\n",
    "            X = add_bias(X_train, self.bias)\n",
    "\n",
    "        (N, m) = X.shape\n",
    "        \n",
    "        self.weights = weights = np.zeros(m)\n",
    "        \n",
    "        if (X_val is None) or (t_val is None): \n",
    "            for e in range(epochs):\n",
    "                weights -= eta / N *  X.T @ (X @ weights - t_train)  \n",
    "    \n",
    "        else:\n",
    "            self.loss = np.zeros(epochs)\n",
    "            self.accuracies = np.zeros(epochs)\n",
    "            for e in range(epochs):\n",
    "                weights -= eta / N *  X.T @ (X @ weights - t_train)   \n",
    "                self.loss[e] =          MSE(X @ self.weights, t_train)\n",
    "                self.accuracies[e] =    accuracy(self.predict(X_val), t_val)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        if self.bias:\n",
    "            X = add_bias(X, self.bias)\n",
    "\n",
    "        ys = X @ self.weights\n",
    "        return ys > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd243ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLogReg():\n",
    "\n",
    "\n",
    "    def __init__(self, bias=-1, dim_hidden=0):\n",
    "        self.bias=bias\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, t_train, X_val=None, t_val=None, eta = 0.1, epochs=10, tol=0.01, n_epochs_no_update=5):\n",
    "        \n",
    "        (N, m) = X_train.shape\n",
    "        if self.bias:\n",
    "            X = add_bias(X_train, self.bias)\n",
    "        \n",
    "\n",
    "        self.weights = weights = np.zeros(m+1)\n",
    "        \n",
    "\n",
    "        if (X_val is None) or (t_val is None):       \n",
    "            for e in range(epochs):\n",
    "                weights -= eta / N *  X.T @ (self.forward(X) - t_train)  \n",
    "        \n",
    "        else:\n",
    "            self.loss = np.zeros(epochs)\n",
    "            self.val_loss = np.zeros(epochs)\n",
    "            self.accuracies = np.zeros(epochs)\n",
    "            self.val_accuracies = np.zeros(epochs)\n",
    "            self.epochs_ran = epochs\n",
    "            # loop trough first n_epochs_no_update\n",
    "            for e in range(n_epochs_no_update):\n",
    "                weights -= eta / N *  X.T @ (self.forward(X) - t_train)      \n",
    "                self.loss[e] =          CE(self.predict_probability(X_train), t_train)\n",
    "                self.val_loss[e] =      CE(self.predict_probability(X_val), t_val)\n",
    "                self.val_accuracies[e]= accuracy(self.predict(X_val), t_val)\n",
    "                self.accuracies[e] =    accuracy(self.predict(X_train), t_train)\n",
    "            # loop trough rest\n",
    "            for e in range(n_epochs_no_update, epochs):\n",
    "                weights -= eta / N *  X.T @ (self.forward(X) - t_train)      \n",
    "                self.loss[e] =          CE(self.predict_probability(X_train), t_train)\n",
    "                self.val_loss[e] =      CE(self.predict_probability(X_val), t_val)\n",
    "                self.val_accuracies[e] =accuracy(self.predict(X_val), t_val)\n",
    "                self.accuracies[e] =    accuracy(self.predict(X_train), t_train)\n",
    "                # if no update exit training\n",
    "                if self.loss[e-n_epochs_no_update] - self.loss[e] < tol:\n",
    "                    self.epochs_ran = e\n",
    "                    self.loss = self.loss[:e]\n",
    "                    self.val_loss = self.val_loss[:e]\n",
    "                    self.val_accuracies = self.val_accuracies[:e]\n",
    "                    self.accuracies = self.accuracies[:e]\n",
    "                    break\n",
    "\n",
    "            \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return logistic(X @ self.weights)\n",
    "    \n",
    "\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        if self.bias:\n",
    "            x = add_bias(x,self.bias)\n",
    "        return (self.forward(x) > threshold).astype('int')\n",
    "    \n",
    "    def predict_probability(self, x):\n",
    "        if self.bias:\n",
    "            x = add_bias(x,self.bias)\n",
    "        return (self.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f43b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLogRegMulti():\n",
    "\n",
    "\n",
    "    def __init__(self, bias=-1, dim_hidden=0):\n",
    "        self.bias=bias\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, t_train, X_val=None, t_val=None, eta = 0.1, epochs=10, tol=0.01, n_epochs_no_update=5):\n",
    "        \n",
    "        (N, m) = X_train.shape\n",
    "        t = one_hot_encoding(t_train)\n",
    "\n",
    "        if self.bias:\n",
    "            X = add_bias(X_train, self.bias)\n",
    "        \n",
    "\n",
    "        self.weights = weights = np.zeros((m+1, t.shape[1]))\n",
    "        \n",
    "\n",
    "        if (X_val is None) or (t_val is None):      \n",
    "            for e in range(epochs):\n",
    "                weights -= eta / N *  X.T @ (self.forward(X) - t)  \n",
    "\n",
    "\n",
    "        else: # if validation is provided: calculate loss and accuracies for each epoch\n",
    "            t_val_t = one_hot_encoding(t_val)\n",
    "            self.loss = np.zeros(epochs)\n",
    "            self.val_loss = np.zeros(epochs)\n",
    "            self.accuracies = np.zeros(epochs)\n",
    "            self.val_accuracies = np.zeros(epochs)\n",
    "            self.epochs_ran = epochs\n",
    "            # loop trough first n_epochs_no_update\n",
    "            for e in range(n_epochs_no_update):\n",
    "                weights -= eta / N *  X.T @ (self.forward(X) - t)      \n",
    "                self.loss[e] =          sum(CE(self.predict_probability(X_train), t))\n",
    "                self.val_loss[e] =      sum(CE(self.predict_probability(X_val), t_val_t))\n",
    "                self.val_accuracies[e]= accuracy(self.predict(X_val), t_val)\n",
    "                self.accuracies[e] =    accuracy(self.predict(X_train), t_train)\n",
    "            # loop trough rest\n",
    "            for e in range(n_epochs_no_update, epochs):\n",
    "                weights -= eta / N *  X.T @ (self.forward(X) - t)      \n",
    "                self.loss[e] =          sum(CE(self.predict_probability(X_train), t))\n",
    "                self.val_loss[e] =      sum(CE(self.predict_probability(X_val), t_val_t))\n",
    "                self.val_accuracies[e] =accuracy(self.predict(X_val), t_val)\n",
    "                self.accuracies[e] =    accuracy(self.predict(X_train), t_train)\n",
    "                # if no update exit training\n",
    "                if self.loss[e-n_epochs_no_update] - self.loss[e] < tol:\n",
    "                    self.epochs_ran = e\n",
    "                    self.loss = self.loss[:e]\n",
    "                    self.val_loss = self.val_loss[:e]\n",
    "                    self.val_accuracies = self.val_accuracies[:e]\n",
    "                    self.accuracies = self.accuracies[:e]\n",
    "                    break\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return logistic(X @ self.weights)\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        if self.bias:\n",
    "            x = add_bias(x,self.bias)\n",
    "        return self.forward(x).argmax(axis=1)\n",
    "    \n",
    "    def predict_probability(self, x):\n",
    "        if self.bias:\n",
    "            x = add_bias(x,self.bias)\n",
    "        return (self.forward(x))\n",
    "    \n",
    "\n",
    "    def score(self,x,t):\n",
    "        return accuracy(self.predict(x), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74b4ca7-7643-49ac-8e6e-502e00c2f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (linreg):  0.8853271991907216\n",
      "        0      1      2      3\n",
      "0  23.624  1.391  1.068  0.726\n",
      "1  11.550  1.405  0.835  0.671\n",
      "2   6.413  1.002  1.135  0.942\n",
      "3   1.480  1.071  0.880  0.885\n",
      "\n",
      "min mse: 0.671\n"
     ]
    }
   ],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(2020)\n",
    "\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "momentum = [0.0, 0.1, 0.5, 0.9]\n",
    "change = 0.0\n",
    "mse = []\n",
    "\n",
    "# the number of datapoints\n",
    "for i in range(len(momentum)):\n",
    "    mse_tmp = []\n",
    "    for j in range(len(learning_rate)):\n",
    "        polydegree = 2\n",
    "        n = 100\n",
    "        x = 2*np.random.rand(n,1)\n",
    "        y = 4+3*x**2+np.random.randn(n,1)\n",
    "\n",
    "        X = np.c_[np.ones((n,1)), x, x**2] \n",
    "\n",
    "        beta = np.random.randn(polydegree+1,1)\n",
    "\n",
    "        eta = learning_rate[j]#learning rate\n",
    "        Niterations = 1000\n",
    "\n",
    "        for iter in range(Niterations):\n",
    "            gradient = (2.0/n)*X.T @ (X @ beta-y)\n",
    "            change = eta*gradient + momentum[i]*change\n",
    "            beta -= change\n",
    "\n",
    "        \n",
    "        ypredict = X.dot(beta)\n",
    "\n",
    "        mse_tmp.append(np.round((1.0/n)*np.sum((y - X@beta)**2), 3))\n",
    "        \n",
    "    mse.append(mse_tmp)\n",
    "\n",
    "beta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "mse_predict = (1.0/n)*np.sum((y - X@beta_linreg)**2)\n",
    "print(\"MSE (linreg): \", mse_predict)\n",
    "\n",
    "panda = pd.DataFrame(data=mse)\n",
    "print(panda)\n",
    "print(\"\\nmin mse:\",np.min(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89a432-503c-4432-abad-ebdc02bddb38",
   "metadata": {},
   "source": [
    "We see that a low learning rate results in bad MSEs probably due to a relatively low number of iterations and it not converging. With higher momentum it drastically improves while the higher learning rates at best get minor improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff75ade-08c6-44d6-84b3-ea5ddf77c242",
   "metadata": {},
   "source": [
    "\n",
    "### Ridge gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314674c2-8c8e-4d94-a415-270540da29e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (linreg):  0.9188629578817374\n",
      "momentum =  0.0\n",
      "       0      1      2      3      4\n",
      "0  1.280  1.134  1.758  1.899  3.807\n",
      "1  1.257  0.894  0.817  1.293  3.992\n",
      "2  1.011  0.987  1.113  1.066  3.738\n",
      "3  0.948  0.912  0.994  0.922  4.039\n",
      "momentum =  0.1\n",
      "       0      1      2      3      4\n",
      "0  1.180  1.219  1.076  0.866  4.072\n",
      "1  1.165  1.219  0.852  1.097  3.697\n",
      "2  1.133  1.146  0.861  0.864  4.150\n",
      "3  0.993  0.974  0.949  1.322  3.527\n",
      "momentum =  0.5\n",
      "       0      1      2      3      4\n",
      "0  1.275  1.239  1.087  1.422  3.910\n",
      "1  1.126  1.116  1.099  1.069  4.459\n",
      "2  1.064  0.983  0.900  1.134  3.664\n",
      "3  0.840  0.959  0.963  0.943  3.999\n",
      "momentum =  0.9\n",
      "       0      1      2      3      4\n",
      "0  1.048  1.687  1.173  1.715  4.238\n",
      "1  1.000  1.003  0.745  1.239  3.723\n",
      "2  0.756  1.128  0.933  1.083  3.775\n",
      "3  1.001  0.777  0.694  0.950  3.903\n",
      "\n",
      "min mse: 0.694\n"
     ]
    }
   ],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "learning_rate = [ 0.001, 0.01, 0.1, 0.25]\n",
    "momentum = [0.0, 0.1, 0.5, 0.9]\n",
    "change = 0.0\n",
    "mse = []\n",
    "lamda_list = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "\n",
    "# the number of datapoints\n",
    "for i in range(len(momentum)):\n",
    "    mse_tmp = []\n",
    "    for j in range(len(learning_rate)):\n",
    "        mse_tmp_tmp = []\n",
    "        for k in range(len(lamda_list)):\n",
    "            polydegree = 2\n",
    "            n = 100\n",
    "            x = 2*np.random.rand(n,1)\n",
    "            y = 4+3*x**2+np.random.randn(n,1)\n",
    "\n",
    "            X = np.c_[np.ones((n,1)), x, x**2] \n",
    "\n",
    "            beta = np.random.randn(polydegree+1,1)\n",
    "\n",
    "            eta = learning_rate[j]#learning rate\n",
    "            Niterations = 1000\n",
    "\n",
    "            lamda = lamda_list[k]\n",
    "\n",
    "            for iter in range(Niterations):\n",
    "                gradient = (2.0/n)*X.T@(X@beta - y) + 2*lamda*beta\n",
    "                change = eta*gradient + momentum[j]*change\n",
    "                beta -= change\n",
    "\n",
    "            ypredict = X.dot(beta)\n",
    "\n",
    "            mse_tmp_tmp.append(np.round((1.0/n)*np.sum((y - X@beta)**2), 3))\n",
    "        \n",
    "        mse_tmp.append(mse_tmp_tmp)\n",
    "        \n",
    "    mse.append(mse_tmp)\n",
    "\n",
    "beta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "mse_predict = (1.0/n)*np.sum((y - X@beta_linreg)**2)\n",
    "print(\"MSE (linreg): \", mse_predict)\n",
    "\n",
    "for i in range(len(mse)):\n",
    "    panda = pd.DataFrame(data=mse[i])\n",
    "    print(\"momentum = \", momentum[i])  \n",
    "    print(panda)\n",
    "\n",
    "print(\"\\nmin mse:\",np.min(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535dbed-2aa4-4c89-90f8-4defb2cccc18",
   "metadata": {},
   "source": [
    "Skrive noe her om det over:::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497bf0a-f07c-4020-8c85-9325c7fa8a58",
   "metadata": {},
   "source": [
    "### OLS stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5da199f6-acbe-437d-8457-907ee589b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (linreg):  0.8742291237044855\n",
      "          0\n",
      "0  1.382672\n",
      "1  0.893757\n",
      "2  0.931771\n",
      "3  0.907148\n",
      "4  0.909486\n",
      "5  0.920278\n",
      "\n",
      "min mse: 0.8937572347487786\n"
     ]
    }
   ],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mse = []\n",
    "epochs = [1, 10, 50, 100, 500, 1000]\n",
    "\n",
    "polydegree = 2\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x**2+np.random.randn(n,1)\n",
    "X = np.c_[np.ones((n,1)), x, x**2] \n",
    "\n",
    "H = (2.0/n)* X.T @ X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "\n",
    "for n_epochs in epochs:\n",
    "\n",
    "    beta = np.random.randn(polydegree+1,1)\n",
    "    eta = 0.01/np.max(EigValues)\n",
    "    M = 5   #size of each minibatch\n",
    "    m = int(n/M) #number of minibatches\n",
    "    t0, t1 = 5, 50\n",
    "\n",
    "    def learning_schedule(t):\n",
    "        return t0/(t+t1)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "    # selects a random mini-batch at every epoch. it does not garanty that all the data will be used\n",
    "        for i in range(m):\n",
    "            random_index = M*np.random.randint(m)\n",
    "            xi = X[random_index:random_index+M]\n",
    "            yi = y[random_index:random_index+M]\n",
    "            gradients = (2.0/M)* xi.T @ ((xi @ beta)-yi)\n",
    "            eta = learning_schedule(epoch*m+i)\n",
    "            beta = beta - eta*gradients\n",
    "    \n",
    "    mse.append((1.0/n)*np.sum((y - X@beta)**2))\n",
    "\n",
    "beta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "mse_predict = (1.0/n)*np.sum((y - X@beta_linreg)**2)\n",
    "print(\"MSE (linreg): \", mse_predict)\n",
    "\n",
    "panda = pd.DataFrame(data=mse)\n",
    "print(panda)\n",
    "\n",
    "print(\"\\nmin mse:\",np.min(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2425d7-a3f9-47df-b2b6-4ad6b858594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skrive noe her om det over:::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886be63-4644-46a2-bc16-eaa95c4aa1fd",
   "metadata": {},
   "source": [
    "### Ridge stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98280720-dd8f-4201-a6db-0d08b650bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (linreg):  0.9901289773595197\n",
      "mse.hape (5, 5)\n",
      "          0         1         2         3         4\n",
      "0  1.093200  1.108377  1.037154  1.031901  0.994656\n",
      "1  1.174978  1.095579  1.144712  1.003394  1.011738\n",
      "2  1.283342  0.995774  1.029529  1.014229  1.021625\n",
      "3  1.150841  1.167716  1.166153  1.166359  1.166051\n",
      "4  3.762844  3.758817  3.758817  3.758817  3.758817\n",
      "\n",
      "min mse: 0.9946562652697165\n"
     ]
    }
   ],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mse = []\n",
    "epochs = [1, 10, 50, 100, 500]\n",
    "\n",
    "mse = []\n",
    "lamda_list = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "polydegree = 2\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x**2+np.random.randn(n,1)\n",
    "X = np.c_[np.ones((n,1)), x, x**2] \n",
    "\n",
    "H = (2.0/n)* X.T @ X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "\n",
    "for lamda in lamda_list:\n",
    "    mse_tmp = []\n",
    "    for n_epochs in epochs:\n",
    "        beta = np.random.randn(polydegree+1,1)\n",
    "        eta = 0.01/np.max(EigValues)\n",
    "        M = 5   #size of each minibatch\n",
    "        m = int(n/M) #number of minibatches\n",
    "        t0, t1 = 5, 50\n",
    "\n",
    "        def learning_schedule(t):\n",
    "            return t0/(t+t1)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "        # selects a random mini-batch at every epoch. it does not garanty that all the data will be used\n",
    "            for i in range(m):\n",
    "                random_index = M*np.random.randint(m)\n",
    "                xi = X[random_index:random_index+M]\n",
    "                yi = y[random_index:random_index+M]\n",
    "                gradients = (2.0/n)*X.T@(X@beta - y) + 2*lamda*beta\n",
    "                eta = learning_schedule(epoch*m+i)\n",
    "                beta = beta - eta*gradients\n",
    "        \n",
    "        mse_tmp.append((1.0/n)*np.sum((y - X@beta)**2))\n",
    "    mse.append(mse_tmp)\n",
    "\n",
    "beta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "mse_predict = (1.0/n)*np.sum((y - X@beta_linreg)**2)\n",
    "print(\"MSE (linreg): \", mse_predict)\n",
    "\n",
    "print(\"mse.hape\", np.shape(mse))\n",
    "\n",
    "panda = pd.DataFrame(data=mse)\n",
    "print(panda)\n",
    "\n",
    "print(\"\\nmin mse:\",np.min(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
