{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from jax import grad\n",
    "\n",
    "\n",
    "def logistic_diff(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def cross_entropy(y, t):\n",
    "    \"\"\"\n",
    "    Cross entropy\n",
    "    \"\"\"\n",
    "    return -np.mean(t * np.log(y) + (1 - t) * np.log(1 - y))\n",
    "\n",
    "\n",
    "def MSE(y, t):\n",
    "    \"\"\"\n",
    "    Mean squared error\n",
    "    \"\"\"\n",
    "    return 0.5 * np.mean((y - t)**2)\n",
    "\n",
    "\n",
    "\n",
    "def loss(w0,w1, b0, b1, X, T):\n",
    "    \"\"\"\n",
    "    Loss function\n",
    "    \"\"\"\n",
    "    a0 = logistic(X @ w0 + b0)\n",
    "    a1 = logistic(a0 @ w1 + b1)\n",
    "    return cross_entropy(a1, T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NN():\n",
    "    \"\"\"\n",
    "    Neural network with one hidden layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_hidden = 6, eta=0.001, epochs = 100, tol=0.01, n_epochs_no_update=10):\n",
    "        # Intialize the hyperparameters\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.n_epochs_no_update = n_epochs_no_update\n",
    "\n",
    "        self.activ = logistic\n",
    "        self.activ_diff = logistic_diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X_train, t_train, X_val=None, t_val=None,  batch_size=5):\n",
    "        (N, m) = X_train.shape\n",
    "        batches = int(N/batch_size)\n",
    "        dim_in = m \n",
    "        dim_out = t_train.shape[1]\n",
    "        self.init_weights_and_biases(dim_in, dim_out)\n",
    "\n",
    "        if (X_val is None) or (t_val is None): \n",
    "            for e in range(self.epochs):\n",
    "                for _ in range(batches):\n",
    "                    random_index = batch_size*onp.random.randint(batches)\n",
    "                    w0g, b0g, w1g, b1g = self.backpropagation\\\n",
    "                    (\n",
    "                        X_train[random_index:random_index+batch_size],\n",
    "                        t_train[random_index:random_index+batch_size],\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            self.loss = np.zeros(self.epochs)\n",
    "            self.accuracies = np.zeros(self.epochs)\n",
    "\n",
    "            for e in range(self.epochs):\n",
    "                for _ in range(batches):\n",
    "                    random_index = batch_size*onp.random.randint(batches)\n",
    "                    w0g, b0g, w1g, b1g = self.backpropagation\\\n",
    "                    (\n",
    "                        X_train[random_index:random_index+batch_size],\n",
    "                        t_train[random_index:random_index+batch_size],\n",
    "                    )\n",
    "                    self.learn(w0g, b0g, w1g, b1g)\n",
    "                self.loss[e] = MSE(self.weights, X_val, t_val)\n",
    "                # self.accuracies[e]= accuracy(self.predict(X_val), t_val)\n",
    "\n",
    "\n",
    "                if e > self.n_epochs_no_update and np.abs(self.loss[e-self.n_epochs_no_update] - self.loss[e]) < self.tol:\n",
    "                    self.loss[e:] = self.loss[e]\n",
    "                    print(f\"Early stopping at epoch {e}\")\n",
    "                    return\n",
    "                print(\"\\rDid not converge\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        returns ( hidden layer, output layer )\n",
    "        \"\"\"\n",
    "        a0 = self.activ(X @ self.w0 + self.b0)  # hidden layer\n",
    "        a1 = self.activ(a0 @ self.w1 + self.b1) # output layer\n",
    "        return a0, a1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        forw = self.forward(X)\n",
    "        return np.argmax(forw[1], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights_and_biases(self, dim_in, dim_out):\n",
    "        self.w0 = onp.random.randn(dim_in, self.dim_hidden)\n",
    "        self.b0 = np.zeros(self.dim_hidden) + 0.01\n",
    "        self.w1 = onp.random.randn(self.dim_hidden, dim_out)\n",
    "        self.b1 = np.zeros(dim_out) + 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def backpropagation(self, X, T ):\n",
    "        \"\"\"\n",
    "        Backpropagation algorithm\n",
    "        ## Parameters\n",
    "            X, T : ndarray\n",
    "                input data and targets\n",
    "        ## Returns\n",
    "            :tuple\n",
    "            ( first layer weights gradient, first layer bias gradient, last layer weights gradient, last layer bias gradient )\n",
    "        \"\"\"\n",
    "        a0, a1 = self.forward(X)\n",
    "        output_error = (a1 - T)\n",
    "        hidden_error =  output_error @ self.w1.T * self.activ_diff(a0)\n",
    "\n",
    "        w0_gradient = X.T @ hidden_error                # * 1/X.shape[0] # can be baked into the learning rate\n",
    "        b0_gradient = np.sum(hidden_error, axis=0)      # * 1/X.shape[0] # can be baked into the learning rate\n",
    "        w1_gradient = a0.T @ output_error               # * 1/X.shape[0] # can be baked into the learning rate\n",
    "        b1_gradient = np.sum(output_error, axis=0)      # * 1/X.shape[0] # can be baked into the learning rate\n",
    "\n",
    "        return w0_gradient, b0_gradient, w1_gradient, b1_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0, 0], \n",
    "                [0, 1], \n",
    "                [1, 0], \n",
    "                [1, 1]  ])\n",
    "\n",
    "\n",
    "t_xor = np.array([  [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]  ])\n",
    "\n",
    "t_and = np.array([  [0],\n",
    "                    [0],\n",
    "                    [0],\n",
    "                    [1]  ])\n",
    "\n",
    "t_or = np.array([   [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [1]  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "nn = NN()\n",
    "\n",
    "nn.fit(X, t_xor)\n",
    "\n",
    "prediction = nn.predict(X)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
