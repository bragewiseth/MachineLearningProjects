{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NN():\n",
    "    \"\"\"\n",
    "    A neural network with one hidden layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_hidden = 6, eta=0.001, epochs = 100,  tol=0.01, n_epochs_no_update=10):\n",
    "        # Intialize the hyperparameters\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.n_epochs_no_update = n_epochs_no_update\n",
    "        \n",
    "        self.activ = logistic\n",
    "        self.activ_diff = logistic_diff\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X_train, t_train, X_val=None, t_val=None,  batch_size=5):\n",
    "        (N, m) = X_train.shape\n",
    "        batches = int(N/batch_size)\n",
    "        dim_in = m \n",
    "        dim_out = t_train.shape[1]\n",
    "        self.init_weights_and_biases(dim_in, dim_out)\n",
    "        \n",
    "        if (X_val is None) or (t_val is None): \n",
    "            for e in range(self.epochs):\n",
    "                for _ in range(batches):\n",
    "                    random_index = batch_size*np.random.randint(batches)\n",
    "                    self.backpropagation(\n",
    "                        X_train[random_index:random_index+batch_size], \n",
    "                        t_train[random_index:random_index+batch_size], N\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            self.loss = np.zeros(self.epochs)\n",
    "            self.accuracies = np.zeros(epochs)\n",
    "\n",
    "            for e in range(self.epochs):\n",
    "                for _ in range(batches):\n",
    "                    random_index = batch_size*np.random.randint(batches)\n",
    "                    self.backpropagation( \n",
    "                        X_train[random_index:random_index+batch_size],\n",
    "                        t_train[random_index:random_index+batch_size], N\n",
    "                    )\n",
    "                self.loss[e] = MSE(self.weights, X_val, t_val)\n",
    "                self.accuracies[e]= accuracy(self.predict(X_val), t_val)\n",
    "\n",
    "\n",
    "                if e > self.n_epochs_no_update and np.abs(self.loss[e-self.n_epochs_no_update] - self.loss[e]) < self.tol:\n",
    "                    self.loss[e:] = self.loss[e]\n",
    "                    print(f\"Early stopping at epoch {e}\")\n",
    "                    return\n",
    "                print(\"\\rDid not converge\")\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_activations = self.activ(X @ self.weights1)\n",
    "        outputs = self.activ(hidden_activations @ self.weights2)\n",
    "        return hidden_activations, outputs\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        forw = self.forward(X)[1]\n",
    "        score= forw[:, 0]\n",
    "        return (score > 0.5).astype('int')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        return self.forward(Z)[1][:, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights_and_biases(self, dim_in, dim_out):\n",
    "        self.w0 = np.random.randn(dim_in, self.dim_hidden)\n",
    "        self.b0 = np.zeros(self.dim_hidden) + 0.01\n",
    "        self.w1 = np.random.randn(self.dim_hidden, dim_out)\n",
    "        self.b1 = np.zeros(dim_out) + 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def backpropagation(self, X, T, N):\n",
    "        hidden_outs, outputs = self.forward(X)\n",
    "        out_deltas = (outputs - T)\n",
    "        hiddenout_diffs = out_deltas @ self.weights2.T\n",
    "        hiddenact_deltas = (hiddenout_diffs[:, 1:] * self.activ_diff(hidden_outs[:, 1:])) # first index is bias hence [:, 1:]\n",
    "\n",
    "        self.w1 -= self.eta * hidden_outs.T @ out_deltas\n",
    "        self.w0 -= self.eta * X.T @ hiddenact_deltas \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
