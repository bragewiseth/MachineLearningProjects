\documentclass[twoside,11pt]{report}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
% \usepackage[utf8]{inputenc}%
% \usepackage{tikz}
% \usepackage{cfr-lm}%
\usepackage[T1]{fontenc}%
\usepackage{physics}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage[margin=3cm]{geometry}
% \usepackage{changepage}
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{lettrine}
% \usepackage{fontawesome}
\usemintedstyle{perldoc}
\usepackage{hyperref}
\hypersetup{colorlinks=false, pdfborder={0 0 0},  }
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{listofitems} % for \readlist to create arrays



\newtcbox{\codebox}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1pt, boxsep=0pt,left=2pt,right=2pt,top=1pt,bottom=.5pt}
\newtcbox{\deloppg}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-2pt]{0pt}{0pt}},boxrule=0pt, boxsep=0pt,left=.49\linewidth,right=.49\linewidth,top=4pt,bottom=3pt}


\newcommand\blfootnote[1]{ \begingroup \renewcommand\thefootnote{}\footnote{#1} \addtocounter{footnote}{-1} \endgroup }
% \definecolor{antwhite}{HTML}{323333}
\newcommand{\code}[3][]{\codebox{\mintinline[#1]{#2}{#3}}}



% \setmainfont{FreeSans}
% \setmainfont{SF Pro Display}
% \setmainfont{IBM Plex Sans}
% \setmainfont{TeX Gyre Heros}
% \setmainfont{Inter}
% \setmainfont{Iosevka Quasi}
% \setmainfont{DM Sans}

% \setmonofont{Iosevka Custom Extended}
% \setmonofont{JetBrainsMono Nerd Font}
\setmonofont[Scale=MatchLowercase]{DM Mono}





% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{https://github.com/bragewiseth/MachineLearningProjects}

% Short headings should be running head and authors last names

\ShortHeadings{\url{https://github.com/bragewiseth/MachineLearningProjects}}{\url{https://github.com/bragewiseth/MachineLearningProjects}}
\firstpageno{1}



\title{{\huge Project 2}}
\author{\name Brage W. \email bragewi@ifi.uio.no\\
    \name Felix C. H.  \email felixch@ifi.uio.no \\
\name Eirik B. J. \email eiribja@ifi.uio.no}
\date{\today}											% Date
\makeatletter






% \date{\today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
    \centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{uio.jpg}\\[1.0 cm]	% University Logo
    \textsc{\LARGE University of Oslo}\\[2.0 cm]	    % University Name
    \textsc{\Large FYS-STK3155}\\[0.5 cm]				% Course Code
    \rule{\linewidth}{0.2 mm} \\[0.4 cm]
    { \huge \bfseries \@title}\\
    \rule{\linewidth}{0.2 mm} \\[1.5 cm]

    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \normalsize
            Brage Wiseth\\
            Felix Cameren Heyerdahl\\
            Eirik Bj√∏rnson Jahr\\
        \end{flushleft}
    \end{minipage}~
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \normalsize
            \textsc{
                bragewi@ifi.uio.no\\
                felixch@ifi.uio.no\\
                eiribja@ifi.uio.no\\
            }
        \end{flushright}

    \end{minipage}\\[2 cm]
    \@date\\
    \vspace*{25mm}
    \urlstyle{rm}
    \textsc{\url{https://github.com/bragewiseth/MachineLearningProjects}}







\end{titlepage}
\nocite{*}
% \maketitle
\newpage
\tableofcontents
\newpage




\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
    \lettrine{I}{}n this paper, we investigate the viability of using neural networks to solve both
    regression and classification problems. Specifically, we compare the performance of neural networks
    with that of traditional linear and logistic regression models. There are many real world applications
    and problems that require function approximation and classification. For example, in the field of
    cancer research, it is often necessary to classify a tumor as either benign or malignant. Often, 
    this is done by extracting a set of features from the tumor and differentiating between the two
    classes based on these features. However, there may be a large degree of overlap between the two and the
    amount of features may be large. These two factors may make it difficult foe a human to classify the tumor.
    By using a neural network, we achieved a classification accuracy of 98\% on the Wisconsin breast cancer
    dataset. With logistic regression, we achieved an accuracy of 98\%.
    In addition to classification, we also investigated the use of neural networks for regression problems.
    We found that neural networks are able to approximate the Franke function with a mean squared error of
    0.0001. This is comparable to the results we achieved with linear regression. However, we found that
    neural networks are much more sensitive to the choice of learning rate and number of epochs. We also
    found that neural networks are much more computationally expensive than linear regression.\\
\end{abstract}
\begin{keywords}
    Regression, Classification, Neural Networks
\end{keywords}









\section{Introduction}




In project 1\cite{MachineLearningProjects_2023}, we found that 
we can fit lines or even polynimals that approximate the distribution of our data by solving an nice analytical expression for the optimal parameters $\beta$. We can in principle approximate 
any function with a polynomial, if we give ourselves infinite degrees of freedom. This is great but there are several
limitations to this approach. First of all, we can not give ourselves infinite degrees of freedom, believe it or not.
Secondly, what if we don't want to find a polynimal but rather classify our data into some classes?\\

\noindent
To tackle classification we can use \emph{logistic regression}, that is, first regression and then clamp the output to a binary value
(for the bianry case). We can do this with an activation function like the sigmoid function
\footnote{The sigmoid function does not output a binary value, but a value between 0 and 1. 
    We can then set a threshold, for example 0.5, and say that if the output is above the threshold, 
    we classify it as 1, and if it is below, we classify it as 0. We can interpret the output as the 
probability of the data point being 1.}
or the heaviside function. However the first problem
still remains, we want to approximate any function, but don't want to use an infinite taylor series. 
We need a different approach.
Instead of finding a single high degree polynimial, we can try to glue together a bunch of smaller line segments.
For this we can use a \emph{neural network}.
As it turns out, the framework for neural networks is very similar to the framework for logistic regression.
Neural networks can be interpeted as several logistic regression models glued together, which is exactly what we wanted!
Another huge benefit of this is that we can use the same code
For both logistic and linear regression as well as neural networks.
This sounds great, but by introducing activation functions we lose the nice analytical expression,
so we can't use the same matrix inversion approach as before.\\
So how do we learn?


\noindent
\textbf{Gradient Decent}:\\ 
    In this section we will discuss the gradient decent algorithm and how it can be used to
    optimize the parameters of a model.\\
\textbf{Data}: \\
    In this section we will discuss the data we will be using in this project.\\
\textbf{Results}:\\
    In this section we will discuss the results we achieved with our models.\\
\textbf{Conclusion}:\\
    In this section we will discuss the results we achieved with our models.\\

\section{Gradient Descent}
\label{sec:GD}

Gradient descent is an iterative optimization algorithm for finding the minimum of a function.
The idea is to take steps in the direction of the negative gradient of the function. It is
verry similar to the Newton-Raphson method for finding roots of a polynimal,
but instead of using the second derivative, we use the first derivative.


\subsection{Backpropagation and Chain Rule}
\label{sec:backpropagation}

Calculating derivatives is the bread and butter of machine learning. For the simpelest models, like linear regression,
the derivatives are relatively easy and straight forward to calculate. We simply take the derivative of the cost function
with respect to our parameters. However, for more complex models, like neural networks, the derivatives 
are not so easy to calculate.
When the loss function is a composition of several functions, we need to invoke the chain rule. 

\begin{figure}[!h]
    \begin{center}
        Forward\\
        \includegraphics[width=\textwidth]{tikzfigures/forward.pdf}\\
    \end{center}
    If we want to calculate the derivative of the loss function with respect to $w_0$,
    we must go trough all the functions in the chain and calculate the derivative of 
    each function with respect to the next function.
    \begin{center}
        Backwards\\
        \hspace{1cm}
        \includegraphics[width=\textwidth]{tikzfigures/backwards.pdf}
    \end{center}
    \caption{chain rule}\label{fig:chainrule}
\end{figure}






\section{Neural Networks}
\label{sec:NN}


\begin{figure}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{tikzfigures/nn.pdf}
    \end{center}
    \caption{A model of a neural network with one hidden layer consisting of four nodes}\label{fig:nn}
    \cite{neutelings_tikzcode}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{tikzfigures/nnActivation.pdf}
    \end{center}
    \caption{}\label{fig:}
    \cite{neutelings_tikzcode}
\end{figure}

With backpropagation we have a way to calculate the derivatives of the loss function with 
respect to the parameters. This allows us to train our neural network. 
with one layer the network becomes a linear regression model. $y = x_0w_0 + x_1w_1 + x_2w_2 + ... + x_nw_n + b_0$ 
with a activation function at the end we can make it into a logistig regression model.
This makes it very convenient to use the same code for both linear and logistic regression as well as for 
larger neural networks with hidden layers, by simply swapping out the different parts.



\section{Data}
\label{sec:data}

For regression we will be using some simple arbitrary function. As to not strain our limited computational resources,
we will only be using one dimension. 

for classification we will be using the Wisconsin breast cancer dataset \cite{wdbc}. This dataset contains 569 data points
with 30 features each. The features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.
They describe characteristics of the cell nuclei present in the image. The features are computed for each cell nucleus:
radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension.
The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image,
resulting in 30 features. For example, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.
All feature values are recoded with four significant digits. Missing attribute values: none.
Class distribution: 357 benign, 212 malignant

the goal is to classify the tumors as either benign or malignant based on the features.
a positive result means that the tumor is malignant, and a negative result means that the tumor is benign.
in other words, we want to find the cases of cancer and classify them as positive

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{../runsAndFigures/feature_histogram.png}
    \end{center}
    \caption{Feature histogram}\label{fig:feature_histogram}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{../runsAndFigures/feature_correlation.png}
    \end{center}
    \caption{Feature correlation. Gives us an idea of the redundancy of the features.
        Only every other features is annotated but you can infer the missing lables 
    from Figure~\ref{fig:feature_histogram}}\label{fig:feature_correlation}
\end{figure}



% The data we will be using consists of 100 points generated using the Franke function. The Franke function is a weighted sum of four exponentials given by
% \begin{align*}
%     f(x,y) &= \frac{3}{4}\exp\left(-\frac{(9x-2)^2}{4}-\frac{(9y-2)^2}{4}\right)\\
%     &+ \frac{3}{4}\exp\left(-\frac{(9x+1)^2}{49}-\frac{(9y+1)}{10}\right)\\
%     &+ \frac{1}{2}\exp\left(-\frac{(9x-7)^2}{4}-\frac{(9y-3)^2}{4}\right)\\
%     &- \frac{1}{5}\exp\left(-(9x-4)^2-(9y-7)^2\right)
% \end{align*}
% We will add some noise to the data to better simulate real world data. The data is generated by sampling $x$ and $y$ from a uniform distribution on the interval $[0,1]$. The noise is sampled from a normal distribution with mean 0 and standard deviation $\sigma = 0.1$.
% \begin{figure}[!h]
% \begin{minipage}[!t]{.48\linewidth}
%     \begin{center}
%         \includegraphics[width=1.1\textwidth]{../runsAndAdditions/trueFunction.png}
%         \caption{True Function}\label{fig:truefunction}
% \end{center}
% \end{minipage}
% \hspace{2mm}
% \begin{minipage}[!t]{.48\linewidth}
%     \begin{center}
%         \includegraphics[width=1.1\textwidth]{../runsAndAdditions/synthDataSide.png}
%         \caption{Our Synthetic Data}\label{fig:synthdataside}
%     \end{center}
% \end{minipage}
% \end{figure}
% \begin{figure}[!h]
% \begin{minipage}[!t]{.48\linewidth}
%     \begin{center}
%         \includegraphics[width=1.0\textwidth]{../runsAndAdditions/realdata3D.png}
%         \caption{True Function}\label{fig:realdata3D}
% \end{center}
% \end{minipage}
% \hspace{2mm}
% \begin{minipage}[!t]{.48\linewidth}
%     \begin{center}
%         \includegraphics[width=1.0\textwidth]{../runsAndAdditions/realdataMap.png}
%         \caption{Our Synthetic Data}\label{fig:realdataMap}
%     \end{center}
% \end{minipage}
% \end{figure}
% The noise is sampled from a normal distribution with mean 0 and standard deviation $\sigma = 0.1$.
% The data is generated by sampling x and y from a uniform distribution on the interval \[0, 1\].
% This synthetic data is divided into training and testing sets, with the testing set comprising twenty percent and the training set comprising eighty percent of the total data.
% In addition, we will incorporate genuine topographic data from a specific area of Norway. However, to reduce computational requirements when running the program, we've reduced the size of this real topographic dataset from 6.5 million points to 10,000 points.


\newpage

\section{Results and Discussion}
\label{sec:resultsdiscussion}


\subsection{Classification}
\label{sec:classification}


\subsubsection{Hyperparameters}
\label{sec:hyperparameters}


\begin{figure}[!h]
\begin{minipage}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{../runsAndFigures/chge.png}
        \caption{Accuracy vs Epochs}\label{fig:}
    \end{center}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{../runsAndFigures/chle.png}
        \caption{Accuracy vs Learning Rate}\label{fig:}
    \end{center}
\end{minipage}
\end{figure}


\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../runsAndFigures/optimizer_accuracy.png}
    \end{center}
    \caption{}\label{fig:}
\end{figure}





\subsubsection{Confusion Matrix}
\label{sec:confusionmatrix}


\subsubsection{Comparisons}
\label{sec:comparisons}

\subsection{Regression}
\label{sec:regression}


\subsubsection{Hyperparameters}
\label{sec:hyperparameters2}



\subsubsection{Comparisons}
\label{sec:comparisons2}


% \section{Bias \& Variance}
% \label{sec:biasvariance}



\section{Conclusion}
\label{sec:conclusion}


Lorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi anim cupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis.




















% \acks{}



% APPENDIX

%
\newpage

\appendix
\phantomsection%
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}
\label{app:appendix}


\section{Universal Approximation}
\label{sec:UAT}

Where is the limit of what we can learn with a neural network? Can we approximate any function?
There are several ways to approximate a function. For periodic functions it may be wise to use a Fourier series,
another option is to use a Taylor series. If these mehtods don't float your boat, we can in fact use a neural network
instead. A neural network with only one hidden layer can approximate any continuous function
given enough neurons in the hidden layer. The more neurons, the higher the resolution of the Approximation
\cite{HornikEtAl89}. 

\phantomsection%
\addcontentsline{toc}{subsection}{XOR VS Perceptron}
\subsection*{XOR VS Perceptron}
\label{app:xor}

A perceptron is a simple model of a neuron. It takes in a set of inputs, multiplies them by a set of weights 
and sums them up to produce an output. The output is then passed through the heaviside
\footnote{We use sigmoid instead of the heaviside, but the outcome of the classification is the same.}
step function to produce
a binary output. The perceptron can be used to solve simple classification problems. However, it is not able to
solve the XOR problem. XOR is not linearly separable, meaning that it is not possible to draw a straight line
that separates the two classes. This is a problem for the perceptron, as it can only draw straight lines.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../runsAndFigures/xor.png}
    \end{center}
    \caption{XOR-variant, it is impossible to separate the two classes with a straight line}\label{fig:xor_data}
\end{figure}


One way to solve this problem is to add some polynomial features to the data. 
This will allow us to draw a curved line that separates the two classes. However, this is not a scalable solution.

\begin{figure}[h]
\begin{minipage}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{../runsAndFigures/xor_plain.png}
        \caption{}\label{fig:}
    \end{center}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{../runsAndFigures/xor_poly.png}
        \caption{}\label{fig:}
    \end{center}
\end{minipage}
\end{figure}




NNs can also learn xor, without the need for a polynomial feature. This is important because it is not 
always obvious what feature engineering is required to solve a problem. Neural networks can learn the 
relevant 
features from the data. Manually adding polynomial features is not a scalable solution for 
high-dimensional data,
whereas neural networks can learn arbitrarily complex functions. Adding polynomial featurer 
is also prone to 
bias problems.


\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../runsAndFigures/xor_nn.png}
    \end{center}
    \caption{}\label{fig:}
\end{figure}



\phantomsection%
\addcontentsline{toc}{subsection}{Combining Neurons}
\subsection*{Combining Neurons}
\label{app:neuronscombined}

We want to approximate this sin function with a neural network. How might that look like?
We can use a single neuron to approximate a line $a w + b$, putting it trough a sigmoid function
we get some non-linearity. Every node in the first hidden layer (and consequents layers) is then 
a line put trough a sigmoid function. The final output of a single hidden layer network is just
a linear combination of these curves. We can add more nodes to get more curves, but we are still


\begin{figure}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{../runsAndFigures/sin.png}
    \end{center}
    \caption{A $\text{sin}$ function that we want to approximate}\label{fig:sin}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{tikzfigures/universal.pdf}
    \end{center}
    \caption{Four indivudual logistic regression outputs scaled and added together to produce 
    a sin approximation}\label{fig:}
\end{figure}


We can chose whatever activation function we want as long as it is non-linear. Relu strugles to learn this 
sin function, but sigmoid works fine.


% \phantomsection%
% \addcontentsline{toc}{subsection}{SVD}
% \subsection*{SVD}
% \label{app:svd}





% \phantomsection%
% \addcontentsline{toc}{subsection}{Confidence Interval}
% \subsection*{Math Behind the Confidence Interval, Section~\ref{sec:confidenceinterval}}
% \label{app:confidenceinterval}




\vskip 0.2in
\bibliography{report}
% \bibliographystyle{apalike}
\bibliographystyle{plain}
\phantomsection%
\addcontentsline{toc}{section}{Bibliography}
\end{document}








