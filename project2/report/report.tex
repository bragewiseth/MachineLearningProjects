\documentclass[twoside,11pt]{report}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
% \usepackage[utf8]{inputenc}%
% \usepackage{tikz}
% \usepackage{cfr-lm}%
\usepackage[T1]{fontenc}%
\usepackage{physics}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage[margin=3cm]{geometry}
% \usepackage{changepage}
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{lettrine}
% \usepackage{fontawesome}
\usemintedstyle{perldoc}
\hypersetup{colorlinks=false, pdfborder={0 0 0},  }
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{tikz}
% \usepackage{listofitems} % for \readlist to create arrays
\usepackage{caption}
\usepackage[toc,page,header]{appendix}


\newtcbox{\codebox}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1pt, boxsep=0pt,left=2pt,right=2pt,top=1pt,bottom=.5pt}
\newtcbox{\deloppg}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-2pt]{0pt}{0pt}},boxrule=0pt, boxsep=0pt,left=.49\linewidth,right=.49\linewidth,top=4pt,bottom=3pt}


\newcommand\blfootnote[1]{ \begingroup \renewcommand\thefootnote{}\footnote{#1} \addtocounter{footnote}{-1} \endgroup }
% \definecolor{antwhite}{HTML}{323333}
\newcommand{\code}[3][]{\codebox{\mintinline[#1]{#2}{#3}}}



% \setmainfont{FreeSans}
% \setmainfont{SF Pro Display}
% \setmainfont{IBM Plex Sans}
% \setmainfont{TeX Gyre Heros}
% \setmainfont{Inter}
% \setmainfont{Iosevka Quasi}
% \setmainfont{DM Sans}

% \setmonofont{Iosevka Custom Extended}
% \setmonofont{JetBrainsMono Nerd Font}
\setmonofont[Scale=MatchLowercase]{DM Mono}





% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{https://github.com/bragewiseth/MachineLearningProjects}

% Short headings should be running head and authors last names

\ShortHeadings{\url{https://github.com/bragewiseth/MachineLearningProjects}}{\url{https://github.com/bragewiseth/MachineLearningProjects}}
\firstpageno{1}



\title{{\huge Project 2}}
\author{\name Brage W. \email bragewi@ifi.uio.no\\
    \name Felix C. H.  \email felixch@ifi.uio.no \\
\name Eirik B. J. \email eiribja@ifi.uio.no}
\date{\today}											% Date
\makeatletter






% \date{\today}

\usepackage{hyperref}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
    \centering
    \vspace*{0.5 cm}
    \includegraphics[scale = 0.75]{uio.jpg}\\[1.0 cm]	% University Logo
    \textsc{\LARGE University of Oslo}\\[2.0 cm]	    % University Name
    \textsc{\Large FYS-STK3155}\\[0.5 cm]				% Course Code
    \rule{\linewidth}{0.2 mm} \\[0.4 cm]
    { \huge \bfseries \@title}\\
    \rule{\linewidth}{0.2 mm} \\[1.5 cm]

    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \normalsize
            Brage Wiseth\\
            Felix Cameren Heyerdahl\\
            Eirik Bj√∏rnson Jahr\\
        \end{flushleft}
    \end{minipage}~
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \normalsize
            \textsc{
                bragewi@ifi.uio.no\\
                felixch@ifi.uio.no\\
                eiribja@ifi.uio.no\\
            }
        \end{flushright}

    \end{minipage}\\[2 cm]
    \@date\\
    \vspace*{25mm}
    \urlstyle{rm}
    \textsc{\url{https://github.com/bragewiseth/MachineLearningProjects}}







\end{titlepage}
\nocite{*}
% \maketitle
\newpage
\tableofcontents
\newpage




\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
    \lettrine{I}{}n this study, we explore the effectiveness of neural networks in solving
    classification and regression problems, contrasting their performance with traditional 
    logistic regression models. Utilizing the Wisconsin breast cancer dataset, we compared the accuracy of 
    these methods in tumor classification. Our results show that neural networks achieved a classification 
    accuracy of 95\%, compared to logistic regression's 96\%. Both are similar to the 96\% achieved using SKLearn's models.
    Additionally, we examined the application of neural networks to regression problems, 
    finding that they can approximate 2-dimensional perlin nose, with a mean squared error of 0.02, 
    compared to linear regression's 0.04. This shows that both neural networks and logistic regression
    are powerful tools for classification tasks, but that neural networks seems to be more viable for complex 
    regression tasks. Offering a more flexible framework for approximating functions
\end{abstract}
\begin{keywords}
    Regression, Classification, Neural Networks
\end{keywords}








\addcontentsline{toc}{section}{Introduction}
\section*{Introduction}

    There are many real world applications and problems that require us to correctly differentiate 
    between some classes. For example, in the field of
    cancer research, it is often necessary to classify a tumor as either benign or malignant. this
    can be done by extracting a set of features from the tumor and differentiating between the two
    classes based on how different classes posess slight variations across the features. 
    However, some features may be more or less equal for both classes, and the
    amount of features may be large. These two factors may make it difficult for a human to classify the tumor.
    To tackle this, we can use \emph{logistic regression}, that is, first regression and then clamp the 
    output to a binary value (for the bianry case). We can do this with an activation function like the sigmoid function
    \footnote
    {
        The sigmoid function does not output a binary value, but a value between 0 and 1. 
        We can then set a threshold, for example 0.5, and say that if the output is above the threshold, 
        we classify it as 1, and if it is below, we classify it as 0. We can interpret the output as the 
        probability of the data point being 1.
    }
    or the heaviside function. This binary value is then the classification of the data point.
    However, in some cases it may be impossible for logistic regression to make a good classification.
    For example, if the data is not linearly separable, linear regression will not be able to draw a
    straight line that separates the two classes. And thus the output of the activation function will
    not be able to classify the data correctly. We explore this problem in more detail in 
    \hyperref[app:appendixB]{Appendix B}. As for now, a solution is using a \emph{neural network} instead.
    Neural networks are able to approximate any continuous function, given enough neurons in the hidden layer.
    This makes them a powerful tool for classification problems. They can also be used for regression problems,
    which we will explore in \hyperref[app:appendixA]{Appendix A}.\\
    
    \noindent
    In project 1\cite{MachineLearningProjects_2023}, we used an analytical expression to find the optimal parameters
    for linear regression. However, by using activation functions we lose our lineariy, and thus we can no longer
    use the same analytical expression to find the optimal parameters. Instead, we must use an iterative approach
    to find the optimal parameters. If our cost function is continuous and differentiable, we can use gradient descent
    to find the minimum of the cost function. All the models we will be using in this project are gradient descent based.


    \noindent
    \textbf{Gradient Decent}:\\ 
    In this section we will discuss the gradient decent algorithm and how it can be used to
    optimize the parameters of a model.\\
    \textbf{Data}: \\
    In this section we will discuss the data we will be using in this project.\\
    \textbf{Results}:\\
    In this section we will discuss the results we achieved with our models.\\
    \textbf{Conclusion}:\\
    In this section we will discuss the results we achieved with our models.\\



\section{Gradient Descent}
\label{sec:GD}

    Gradient descent is an iterative optimization algorithm for finding the minimum of a function.
    The idea is to take steps in the direction of the negative gradient of the function. It is
    verry similar to the Newton-Raphson method for finding roots of a polynimal,
    but instead of using the second derivative, we use the first derivative.

    optimizers

    stocastic





\subsection{Backpropagation and Chain Rule}
\label{sec:backpropagation}

    Calculating derivatives is the bread and butter of machine learning. For the simpelest models, like linear regression,
    the derivatives are relatively easy and straight forward to calculate. We simply take the derivative of the cost function
    with respect to our parameters. However, for more complex models, like neural networks, the derivatives 
    are not so easy to calculate.
    When the loss function is a composition of several functions, we need to invoke the chain rule. 

    \begin{figure}[!h]
        \begin{center}
            Forward\\
            \includegraphics[width=\textwidth]{tikzfigures/forward.pdf}\\
        \end{center}
        If we want to calculate the derivative of the loss function with respect to $w_0$,
        we must go trough all the functions in the chain and calculate the derivative of 
        each function with respect to the next function.
        \begin{center}
            Backwards\\
            \hspace{1cm}
            \includegraphics[width=\textwidth]{tikzfigures/backwards.pdf}
        \end{center}
        \caption{chain rule}\label{fig:chainrule}
    \end{figure}




\section{Neural Networks}
\label{sec:NN}

    As it turns out, the framework for neural networks is very similar to the framework for logistic regression.
    Neural networks can be interpeted as several logistic regression models glued together, 
    huge benefit of this is that we can use the same code
    For both logistic and linear regression as well as neural networks.

    \begin{figure}
        \begin{center}
            \includegraphics[width=0.4\textwidth]{tikzfigures/nn.pdf}
        \end{center}
        \caption{A model of a neural network with one hidden layer consisting of four nodes}\label{fig:nn}
        \cite{neutelings_tikzcode}
    \end{figure}
    \begin{figure}
        \begin{center}
            \includegraphics[width=\textwidth]{tikzfigures/nnActivation.pdf}
        \end{center}
        \caption{}\label{fig:nn_math}
        \cite{neutelings_tikzcode}
    \end{figure}


    With backpropagation we have a way to calculate the derivatives of the loss function with 
    respect to the parameters. This allows us to train our neural network. 
    with one layer the network becomes a linear regression model. $y = x_0w_0 + x_1w_1 + x_2w_2 + ... + x_nw_n + b_0$ 
    with a activation function at the end we can make it into a logistig regression model.
    This makes it very convenient to use the same code for both linear and logistic regression as well as for 
    larger neural networks with hidden layers, by simply swapping out the different parts.


    activation functions




\section{Data}
\label{sec:data}

    for classification we will be using the Wisconsin breast cancer dataset [cite here]. This dataset contains 
    569 data points with 30 features each. The features are computed from a digitized image of a fine needle aspirate
    (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The features are 
    computed for each cell nucleus: radius, texture, perimeter, area, smoothness, compactness, concavity, 
    concave points, symmetry, fractal dimension. The mean, standard error and "worst" or largest 
    (mean of the three largest values) of these features were computed for each image, resulting in 30 features. 
    For example, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.
    All feature values are recoded with four significant digits. Missing attribute values: none.
    Class distribution: 357 benign, 212 malignant

    malignant = cancer.data[cancer.target==0]
    benign = cancer.data[cancer.target==1]

    the goal is to classify the tumors as either benign or malignant based on the features.
    a positive result means that the tumor is malignant, and a negative result means that the tumor is benign.
    in other words, we want to find the cases of cancer and classify them as positive

    \begin{figure}
        \begin{center}
            \includegraphics[width=\textwidth]{../runsAndFigures/feature_histogram.png}
        \end{center}
        \caption{Feature histogram. Red ($0$) class is malignent, green ($1$) is benign}\label{fig:feature_histogram}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics[width=\textwidth]{../runsAndFigures/feature_correlation.png}
        \end{center}
        \caption{Feature correlation. Gives us an idea of the redundancy of the features.
            Only every other features is annotated but you can infer the missing lables 
        from Figure~\ref{fig:feature_histogram}}\label{fig:feature_correlation}
    \end{figure}



\clearpage

\section{Results and Analysis}
\label{sec:resultsdiscussion}

    For classification we use the cross entropy loss function. 
    We tune the hyperparameters in a grid searches with a maximum
    of two dimensions. As we find the optimal hyperparameters, we use the same optimal hyperparameters for the next grid search.

    some of the hyperparameters are more dependent on each other than others. For example, the learning rate and the number of epochs
    the batch size and the learning rate. We can therefore not tune these hyperparameters independently. We must first find the optimal

    due to our limited time and computational resources we have not been able to do a full grid search for all hyperparameters. 
    we train each hyperparameter separatly, in some cases in a grid of two dimensions. 
    we then use this as a utgangspunkt for fine tuning.

\subsection{Hyperparameters}
\label{sec:hyperparameters}
    epochs found to be 200 

    when training for batch size we had to decrease the optimal learning rate due to unstable results for smaller batches

    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/accuracy_lr_gamma.png}
            \end{center}
            \caption
            {
                In this case havving momentum seems to be beneficial. 
                We maxes out our testing range and found that 0.9 was the best value for momentum. 
                momentum allows a higher learning rate
            }\label{fig:accuracy_lr_gamma}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/accuracy_alpha.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:accuracy_aplha}
        \end{minipage}
    \end{figure}


    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/accuracy_optimizer.png}
            \end{center}
            \caption{In this case havving momentum seems to be beneficial. We maxes out our testing range and found that 0.9 was the best value for momentum. momentum allows a higher learning rate}\label{fig:accuracy_optimizer}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/accuracy_batch.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:accuracy_aplha}
        \end{minipage}
    \end{figure}


    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/accuracy_activ.png}
            \end{center}
            \caption
            {
                In this case havving momentum seems to be beneficial. We maxes out our testing range and 
                found that 0.9 was the best value for momentum. momentum allows a higher learning rate
            }\label{fig:accuracy_optimizer}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/accuracy_layers_nodes.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:accuracy_aplha}
        \end{minipage}
    \end{figure}



\subsection{Final Evaluation and Comparisons}
\label{sec:comparisons}


    \begin{figure}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../runsAndFigures/confusion_matrix.png}
        \end{center}
        \caption{}\label{fig:}
    \end{figure}



\section{Conclusion}
\label{sec:conclusion}


    Lorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi anim cupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis.




















% \acks{}


\clearpage 

\appendix
\renewcommand{\theHchapter}{appendix\Alph{chapter}}
\renewcommand{\theHsection}{appendix\thesection}

\phantomsection
\addcontentsline{toc}{chapter}{Appendix}


\chapter*{Appendix A}
\label{app:appendixA}


\section{Regression with Neural Networks}
\label{sec:regression}

    In project 1\cite{MachineLearningProjects_2023}, we found that 
    we can fit lines or even polynimals that approximate the distribution of our data by solving an nice 
    analytical expression for the optimal parameters $\beta$. We can in principle approximate 
    any function with a polynomial, if we give ourselves infinite degrees of freedom. This is great but there are several
    limitations to this approach. First of all, we can not give ourselves infinite degrees of freedom, believe it or not.
    Secondly, what if we don't want to find a polynimal but rather classify our data into some classes?\\
    However the first problem
    still remains, we want to approximate any function, but don't want to use an infinite taylor series. 
    We need a different approach.
    Instead of finding a single high degree polynimial, we can try to glue together a bunch of smaller line segments.
    For this we can use a \emph{neural network}.


\section*{Data}

    For regression we will be using perlin noise to generate our dataset 

    \begin{figure}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{../runsAndFigures/perlinNoise.png}
        \end{center}
        \caption{}\label{fig:perlin}
    \end{figure}

    Solving a regression problem with linear regression and neural networks using gradient optimization.



\section*{Results and Analysis}
\label{sec:resultsdiscussion2}




\subsection*{Hyperparameters}
\label{sec:hyperparameters2}


    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/MSE_lr_gamma.png}
            \end{center}
            \caption{In this case havving momentum seems to be beneficial. We maxes out our testing range and found that 0.9 was the best value for momentum. momentum allows a higher learning rate}\label{fig:MSE_lr_gamma}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/MSE_alpha.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:MSE_aplha}
        \end{minipage}
    \end{figure}


    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/MSE_optimizer.png}
            \end{center}
            \caption{In this case havving momentum seems to be beneficial. We maxes out our testing range and found that 0.9 was the best value for momentum. momentum allows a higher learning rate}\label{fig:MSE_optimizer}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/MSE_batch.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:MSE_batch}
        \end{minipage}
    \end{figure}

    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/MSE_activs.png}
            \end{center}
            \caption{In this case havving momentum seems to be beneficial. We maxes out our testing range and found that 0.9 was the best value for momentum. momentum allows a higher learning rate}\label{fig:accuracy_optimizer}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/MSE_layers_nodes.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:accuracy_aplha}
        \end{minipage}
    \end{figure}


    \begin{figure}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../runsAndFigures/MSE_degree.png}
        \end{center}
        \caption{}\label{fig:}
    \end{figure}



\subsection*{Final Evaluation and Comparisons}
\label{sec:comparisons2}






    \begin{figure}[!ht]
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/perlinNoise_logistic_pred.png}
            \end{center}
            \caption{In this case havving momentum seems to be beneficial. We maxes out our testing range and found that 0.9 was the best value for momentum. momentum allows a higher learning rate}\label{fig:accuracy_optimizer}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}[t]{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/perlinNoise_NN_pred.png}
            \end{center}
            \caption{Introduction regualarization does not seem to yield any benefits, in fact
            having too much regualarization shunts performance}\label{fig:accuracy_aplha}
        \end{minipage}
    \end{figure}











\chapter*{Appendix B}
\label{app:appendixB}


\section{Universal Approximation}
\label{sec:UAT}

    Where is the limit of what we can learn with a neural network? Can we approximate any function?
    There are several ways to approximate a function. For periodic functions it may be wise to use a Fourier series,
    another option is to use a Taylor series. If these mehtods don't float your boat, we can in fact use a neural network
    instead. A neural network with only one hidden layer can approximate any continuous function
    given enough neurons in the hidden layer. The more neurons, the higher the resolution of the Approximation
    \cite{HornikEtAl89}. 

\subsection*{XOR VS Perceptron}
\label{app:xor}

    \begin{wrapfigure}{r}{0.5\textwidth}
        \begin{center}
            \includegraphics[width=0.4\textwidth]{../runsAndFigures/xor.png}
        \end{center}
        \caption{}\label{fig:xor_data}
    \end{wrapfigure}

    A perceptron is a simple model of a neuron. It takes in a set of inputs, multiplies them by a set of weights 
    and sums them up to produce an output. The output is then passed through the heaviside
    \footnote{We use sigmoid instead of the heaviside, but the outcome of the classification is the same.}
    step function to produce
    a binary output. The perceptron can be used to solve simple classification problems. However, it is not able to
    solve the XOR problem. XOR is not linearly separable, meaning that it is not possible to draw a straight line
    that separates the two classes. This is a problem for the perceptron, as it can only draw straight lines.
    One way to solve this problem is to add some polynomial features to the data. 
    This will allow us to draw a curved line that separates the two classes. However, this is not a scalable solution.
    NNs can also learn xor, without the need for a polynomial feature. This is important because it is not 
    always obvious what feature engineering is required to solve a problem. Neural networks can learn the 
    relevant 
    features from the data. Manually adding polynomial features is not a scalable solution for 
    high-dimensional data,
    whereas neural networks can learn arbitrarily complex functions. Adding polynomial featurer 
    is also prone to 
    bias problems.




    \begin{figure}[h]
        \begin{minipage}{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/xor_plain.png}
                \caption{}\label{fig:xor_plain}
            \end{center}
        \end{minipage}
        \hspace{2mm}
        \begin{minipage}{0.5\textwidth - 1mm}
            \begin{center}
                \includegraphics[width=\textwidth]{../runsAndFigures/xor_poly.png}
                \caption{}\label{fig:xor_poly}
            \end{center}
        \end{minipage}
    \end{figure}



    \begin{figure}[!h]
        \begin{center}
            \includegraphics[width=0.5\textwidth]{../runsAndFigures/xor_nn.png}
        \end{center}
        \caption{}\label{fig:xor_nn}
    \end{figure}



\subsection*{Combining Neurons}
\label{app:neuronscombined}

    We want to approximate this sin function with a neural network. How might that look like?
    We can use a single neuron to approximate a line $a w + b$, putting it trough a sigmoid function
    we get some non-linearity. Every node in the first hidden layer (and consequents layers) is then 
    a line put trough a sigmoid function. The final output of a single hidden layer network is just
    a linear combination of these curves. We can add more nodes to get more curves, but we are still
    We can chose whatever activation function we want as long as it is non-linear. Relu strugles to learn this 
    sin function, but sigmoid works fine.


    \begin{figure}
        \begin{center}
            \includegraphics[width=0.4\textwidth]{../runsAndFigures/sin.png}
        \end{center}
        \caption{A $\text{sin}$ function that we want to approximate}\label{fig:sin}
    \end{figure}

    \begin{figure}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{tikzfigures/universal.pdf}
        \end{center}
        \caption{Four indivudual logistic regression outputs scaled and added together to produce 
        a sin approximation}\label{fig:universal}
    \end{figure}






\vskip 0.2in
\bibliography{report}
% \bibliographystyle{apalike}
\bibliographystyle{plain}
% \phantomsection%
\addcontentsline{toc}{section}{Bibliography}
\end{document}








